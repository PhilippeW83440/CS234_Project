{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anti Collision Tests Environment - openai gym compatible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_img(img):\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "def  resize_images(images, f=3):\n",
    "    big_images = []\n",
    "    for img in images:\n",
    "        big_images.append(cv2.resize(img, None, fx=f, fy=f))\n",
    "    return big_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"]=10,10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Some useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dist(obj1, obj2):\n",
    "    return math.sqrt((obj1[0]-obj2[0])**2 + (obj1[1]-obj2[1])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dist_nearest_obj(s):\n",
    "    nobjs = int(len(s)/4 - 1)\n",
    "    ego = s[0:4]\n",
    "    \n",
    "    dist_nearest_obj = math.inf\n",
    "    num_nearest_obj = -1\n",
    "    \n",
    "    idx = 4\n",
    "    for n in range(nobjs):\n",
    "        obj = s[idx:idx+4]\n",
    "        dist = get_dist(ego, obj)\n",
    "        \n",
    "        if dist < dist_nearest_obj:\n",
    "            dist_nearest_obj = dist\n",
    "            num_nearest_obj = n\n",
    "        idx += 4\n",
    "    \n",
    "    return dist_nearest_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dist_to_goal(s, goal):\n",
    "    return get_dist(s[0:4], goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time To Collision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_TTC(ego, obj, radius):\n",
    "    x1, y1, vx1, vy1 = ego[0], ego[1], ego[2], ego[3]\n",
    "    x2, y2, vx2, vy2 = obj[0], obj[1], obj[2], obj[3]\n",
    "\n",
    "    a = (vx1 - vx2) **2 + (vy1 - vy2) **2\n",
    "    b = 2 * ((x1 - x2) * (vx1 - vx2) + (y1 - y2) * (vy1 - vy2))\n",
    "    c = (x1 - x2) **2 + (y1 - y2) **2 - radius **2\n",
    "\n",
    "    if a == 0 and b == 0:\n",
    "        if c == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return np.inf\n",
    "\n",
    "    if a == 0 and b != 0:\n",
    "        t = -c / b\n",
    "        if t < 0:\n",
    "            return np.inf\n",
    "        else:\n",
    "            return t\n",
    "\n",
    "    delta = b **2 - 4 * a * c\n",
    "    if delta < 0:\n",
    "        return np.inf\n",
    "\n",
    "    t1 = (-b - np.sqrt(delta)) / (2 * a)\n",
    "    t2 = (-b + np.sqrt(delta)) / (2 * a)\n",
    "    if t1 < 0:\n",
    "        t1 = np.inf\n",
    "\n",
    "    if t2 < 0:\n",
    "        t2 = np.inf\n",
    "\n",
    "    return min(t1, t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_smallest_TTC(s):\n",
    "    radius = 15.0\n",
    "    ego = s[0:4]\n",
    "    \n",
    "    smallest_TTC = np.Inf\n",
    "    smallest_TTC_obj = -1\n",
    "    \n",
    "    idx = 4\n",
    "    for n in range(int((len(s)-4)/4)):\n",
    "        obj = s[idx:idx+4]\n",
    "        TTC = get_TTC(ego, obj, radius)\n",
    "        \n",
    "        if TTC < smallest_TTC:\n",
    "            smallest_TTC = TTC\n",
    "            smallest_TTC_obj = n\n",
    "        idx += 4\n",
    "    \n",
    "    return smallest_TTC, smallest_TTC_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dist:  141.4213562373095\n",
      "TTC:  4.4696699141100895\n"
     ]
    }
   ],
   "source": [
    "# just checking\n",
    "obj1 = np.array([0.0, 100, 20, 0])\n",
    "obj2 = np.array([100, 0.0, 0, 20])\n",
    "print(\"dist: \", get_dist(obj1, obj2))\n",
    "print(\"TTC: \", get_TTC(obj1, obj2, 15.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Made up ...\n",
    "\n",
    "class BasicDriverModel():\n",
    "    # stationarity: 1 is very aggressive, 4 is not very aggressive\n",
    "    def __init__(self, statio=3.0, dt=0.2):\n",
    "        self.state = \"SPEED_CONSTANT\" # SACCEL SCONSTANT\n",
    "        self.stationarity = statio # every 1, 2, 3 or 4 seconds\n",
    "        self.accel = 0 # -1 0 1 random uniform on ax\n",
    "        self.duration = 0\n",
    "        self.dt = dt\n",
    "        \n",
    "    def step(self):\n",
    "        if self.state == \"SPEED_CONSTANT\":\n",
    "            self.duration += self.dt\n",
    "            if self.duration >= self.stationarity:\n",
    "                self.accel = np.random.randint(low=-1, high=2)\n",
    "                self.state = \"SPEED_CHANGE\"\n",
    "                self.duration = 0\n",
    "        elif self.state == \"SPEED_CHANGE\":\n",
    "            self.duration += self.dt\n",
    "            if self.duration >= self.stationarity:\n",
    "                self.accel = 0\n",
    "                self.state = \"SPEED_CONSTANT\"\n",
    "                self.duration = 0\n",
    "        return self.accel, self.state  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accel 0 state SPEED_CONSTANT\n",
      "accel 0 state SPEED_CONSTANT\n",
      "accel 0 state SPEED_CONSTANT\n",
      "accel 0 state SPEED_CONSTANT\n",
      "accel 0 state SPEED_CONSTANT\n",
      "accel 0 state SPEED_CONSTANT\n",
      "accel 0 state SPEED_CONSTANT\n",
      "accel 0 state SPEED_CONSTANT\n",
      "accel 0 state SPEED_CONSTANT\n",
      "accel 0 state SPEED_CONSTANT\n",
      "accel 1 state SPEED_CHANGE\n",
      "accel 1 state SPEED_CHANGE\n",
      "accel 1 state SPEED_CHANGE\n",
      "accel 1 state SPEED_CHANGE\n",
      "accel 1 state SPEED_CHANGE\n",
      "accel 1 state SPEED_CHANGE\n",
      "accel 1 state SPEED_CHANGE\n",
      "accel 1 state SPEED_CHANGE\n",
      "accel 1 state SPEED_CHANGE\n",
      "accel 1 state SPEED_CHANGE\n"
     ]
    }
   ],
   "source": [
    "driver = BasicDriverModel(statio=2.0)\n",
    "for i in range(20):\n",
    "    accel, state = driver.step()\n",
    "    print(\"accel {} state {}\".format(accel, state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cf https://en.wikipedia.org/wiki/Intelligent_driver_model\n",
    "# cf https://github.com/sisl/AutoUrban.jl/blob/master/src/drivermodels/IDMDriver.jl\n",
    "\n",
    "class IntelligentDriverModel():\n",
    "    def __init__(self, v_des = 29.0):\n",
    "        self.a = None # predicted acceleration\n",
    "        self.sigma = 0 # optional stdev on top of the model, set to zero for deterministic behavior\n",
    "        \n",
    "        self.k_spd = 1.0 # proportional constant for speed tracking when in freeflow [s⁻¹]\n",
    "        \n",
    "        self.delta = 4.0 # acceleration exponent [-]\n",
    "        self.T = 1.5 # desired time headway [s]\n",
    "        self.v_des = v_des # desired speed [m/s], typically overwritten\n",
    "        self.s_min = 5.0 # minimum acceptable gap [m]\n",
    "        self.a_max = 3.0 # maximum acceleration ability [m/s²]\n",
    "        self.d_cmf = 2.0 # comfortable deceleration [m/s²] (positive)\n",
    "        self.d_max = 9.0 # maximum decelleration [m/s²] (positive)\n",
    "        \n",
    "    def step(self, v_ego, v_oth, headway):\n",
    "        if v_oth is not None:\n",
    "            assert headway is not None and headway > 0, \"v_oth None but headway > 0\"\n",
    "            if headway > 0.0:\n",
    "                dv = v_oth - v_ego\n",
    "                s_des = self.s_min + v_ego * self.T - v_ego * dv / (2 * math.sqrt(self.a_max * self.d_cmf))\n",
    "                \n",
    "                if self.v_des > 0.0:\n",
    "                    v_ratio = v_ego / self.v_des\n",
    "                else:\n",
    "                    v_ratio = 1.0\n",
    "                                        \n",
    "                self.a = self.a_max * (1.0 - v_ratio**self.delta - (s_des / headway)**2)\n",
    "            # elseif headway > -3.0\n",
    "            #    model.a = -model.d_max\n",
    "            else:\n",
    "                dv = self.v_des - v_ego\n",
    "                self.a = dv * self.k_spd\n",
    "        else:\n",
    "            # no lead vehicle, just drive to match desired speed\n",
    "            dv = self.v_des - v_ego\n",
    "            self.a = dv * self.k_spd # predicted accel to match target speed\n",
    "\n",
    "        if self.a is None:\n",
    "            print(\"headway: {} v_oth: {} v_ego: {}\".format(headway, v_oth, v_ego))\n",
    "        assert self.a is not None, \"idm accel None\"\n",
    "\n",
    "        self.a = np.clip(self.a, -self.d_max, self.a_max)\n",
    "        \n",
    "        if self.sigma > 0:\n",
    "            self.a += self.sigma * np.random.randn()\n",
    "            self.a = np.clip(self.a, -self.d_max, self.a_max)\n",
    "            \n",
    "        return self.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_ego 10.6 a 3.0\n",
      "v_ego 11.2 a 3.0\n",
      "v_ego 11.799999999999999 a 3.0\n",
      "v_ego 12.399999999999999 a 3.0\n",
      "v_ego 12.999999999999998 a 3.0\n",
      "v_ego 13.599999999999998 a 3.0\n",
      "v_ego 14.199999999999998 a 3.0\n",
      "v_ego 14.799999999999997 a 3.0\n",
      "v_ego 15.399999999999997 a 3.0\n",
      "v_ego 15.999999999999996 a 3.0\n",
      "v_ego 16.599999999999998 a 3.0\n",
      "v_ego 17.2 a 3.0\n",
      "v_ego 17.8 a 3.0\n",
      "v_ego 18.400000000000002 a 3.0\n",
      "v_ego 19.000000000000004 a 3.0\n",
      "v_ego 19.600000000000005 a 3.0\n",
      "v_ego 20.200000000000006 a 3.0\n",
      "v_ego 20.800000000000008 a 3.0\n",
      "v_ego 21.40000000000001 a 3.0\n",
      "v_ego 22.00000000000001 a 3.0\n",
      "v_ego 22.600000000000012 a 3.0\n",
      "v_ego 23.200000000000014 a 3.0\n",
      "v_ego 23.800000000000015 a 3.0\n",
      "v_ego 24.400000000000016 a 3.0\n",
      "v_ego 25.000000000000018 a 3.0\n",
      "v_ego 25.60000000000002 a 3.0\n",
      "v_ego 26.20000000000002 a 3.0\n",
      "v_ego 26.800000000000022 a 3.0\n",
      "v_ego 27.400000000000023 a 3.0\n",
      "v_ego 27.92000000000002 a 2.5999999999999766\n",
      "v_ego 28.336000000000016 a 2.0799999999999805\n",
      "v_ego 28.66880000000001 a 1.6639999999999837\n",
      "v_ego 28.935040000000008 a 1.3311999999999884\n",
      "v_ego 29.148032000000008 a 1.0649599999999921\n",
      "v_ego 29.318425600000005 a 0.8519679999999923\n",
      "v_ego 29.454740480000005 a 0.6815743999999953\n",
      "v_ego 29.563792384000003 a 0.5452595199999948\n",
      "v_ego 29.651033907200002 a 0.43620761599999724\n",
      "v_ego 29.720827125760003 a 0.3489660927999978\n",
      "v_ego 29.776661700608003 a 0.2791728742399968\n",
      "v_ego 29.821329360486402 a 0.22333829939199745\n",
      "v_ego 29.857063488389123 a 0.17867063951359796\n",
      "v_ego 29.8856507907113 a 0.14293651161087695\n",
      "v_ego 29.90852063256904 a 0.11434920928870085\n",
      "v_ego 29.926816506055232 a 0.09147936743095997\n",
      "v_ego 29.941453204844187 a 0.07318349394476797\n",
      "v_ego 29.95316256387535 a 0.05854679515581296\n",
      "v_ego 29.96253005110028 a 0.04683743612465108\n",
      "v_ego 29.970024040880226 a 0.03746994889971944\n",
      "v_ego 29.976019232704182 a 0.02997595911977413\n",
      "v_ego 29.980815386163346 a 0.023980767295817884\n",
      "v_ego 29.984652308930677 a 0.019184613836653597\n",
      "v_ego 29.98772184714454 a 0.015347691069322877\n",
      "v_ego 29.990177477715633 a 0.012278152855458302\n",
      "v_ego 29.992141982172505 a 0.009822522284366642\n",
      "v_ego 29.993713585738004 a 0.007858017827494734\n",
      "v_ego 29.994970868590404 a 0.006286414261996498\n",
      "v_ego 29.995976694872322 a 0.005029131409596488\n",
      "v_ego 29.99678135589786 a 0.004023305127677901\n",
      "v_ego 29.997425084718287 a 0.0032186441021408996\n"
     ]
    }
   ],
   "source": [
    "driver = IntelligentDriverModel(v_des=30.0)\n",
    "v_ego = 10\n",
    "dt = 0.2\n",
    "for i in range(60):\n",
    "    a = driver.step(v_ego, None, None)\n",
    "    v_ego += a*dt\n",
    "    print(\"v_ego {} a {}\".format(v_ego, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Openai gym Anti Collision Test Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################################################\n",
    "# Anti Collision Tests env compatible with gym openai interface\n",
    "##################################################################\n",
    "\n",
    "# API eg https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py\n",
    "\n",
    "stationarity = 4.0\n",
    "\n",
    "\n",
    "import gym\n",
    "from gym import spaces, logger\n",
    "from gym.utils import seeding\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import cv2\n",
    "import copy\n",
    "\n",
    "def draw_arrow(image, p, q, color, arrow_magnitude=5, thickness=1, line_type=4, shift=0):\n",
    "    # adapted from http://mlikihazar.blogspot.com.au/2013/02/draw-arrow-opencv.html\n",
    "    # draw arrow tail\n",
    "    cv2.line(image, p, q, color, thickness, line_type, shift)\n",
    "    # calc angle of the arrow\n",
    "    angle = np.arctan2(p[1]-q[1], p[0]-q[0])\n",
    "    # starting point of first line of arrow head\n",
    "    p = (int(q[0] + arrow_magnitude * np.cos(angle + np.pi/4)),\n",
    "    int(q[1] + arrow_magnitude * np.sin(angle + np.pi/4)))\n",
    "    # draw first half of arrow head\n",
    "    cv2.line(image, p, q, color, thickness, line_type, shift)\n",
    "    # starting point of second line of arrow head\n",
    "    p = (int(q[0] + arrow_magnitude * np.cos(angle - np.pi/4)),\n",
    "    int(q[1] + arrow_magnitude * np.sin(angle - np.pi/4)))\n",
    "    # draw second half of arrow head\n",
    "    cv2.line(image, p, q, color, thickness, line_type, shift)\n",
    "\n",
    "# Transition with Constant Acceleration model\n",
    "def transition_ca(s, a, dt=0.2):\n",
    "    Ts = np.matrix([[1.0, 0.0, dt,  0.0], \n",
    "                [0.0, 1.0, 0.0, dt],\n",
    "                [0.0, 0.0, 1.0, 0.0],\n",
    "                [0.0, 0.0, 0.0, 1.0]])\n",
    "    Ta = np.matrix([[0.5*dt**2, 0.0],\n",
    "                [0.0,      0.5*dt**2],\n",
    "                [dt,       0.0],\n",
    "                [0.0,      dt]])\n",
    "    return np.dot(Ts, s) + np.dot(Ta, a)\n",
    "\n",
    "\n",
    "class ActEnv(gym.Env):\n",
    "    def __init__(self, nobjs, max_accel=2, dist_collision=10, reward_shaping=True):    \n",
    "        self.nobjs = nobjs\n",
    "        self.max_accel = max_accel\n",
    "        self.dist_collision = dist_collision\n",
    "        self.reward_shaping = reward_shaping\n",
    "        \n",
    "        self.action_space = spaces.Box(low=-self.max_accel, high=self.max_accel, shape=(1,))\n",
    "        # 1+nobjs: x,y,vx,vy with x,y in [0,200] and vx,vy in [0,40]\n",
    "        self.observation_space = spaces.Box(low=0.0, high=200.0, shape=((1+nobjs)*4,))\n",
    "        \n",
    "        self.seed()\n",
    "        self.reset()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        print(\"SEED {}\".format(seed))\n",
    "        return [seed]\n",
    "        \n",
    "    def reset(self):\n",
    "        self.reward = None\n",
    "        self.steps_beyond_done = 0\n",
    "        self.steps = 0\n",
    "        self.smallest_TTC_obj = -1\n",
    "        \n",
    "        self.drivers = []\n",
    "        \n",
    "        # x, y, vx, vy\n",
    "        self.start = np.array([100.0,   0.0,  0.0,  20.0], dtype=int)\n",
    "        self.goal  = np.array([100.0, 200.0, 0.0, 0.0], dtype=int)\n",
    "        # states init\n",
    "        state = ego = self.start\n",
    "        for n in range(int(self.nobjs/2)):\n",
    "            x = self.np_random.randint(low=0, high=50)\n",
    "            y = self.np_random.randint(low=25, high=190)\n",
    "            vx = self.np_random.randint(low=10, high=25)\n",
    "            vy = self.np_random.randint(low=0, high=5)\n",
    "            obj = np.array([x, y, vx, vy])\n",
    "            state = np.append(state, obj)\n",
    "            driver = BasicDriverModel(statio=stationarity)\n",
    "            self.drivers.append(driver)\n",
    "        \n",
    "        for n in range(int(self.nobjs/2)):\n",
    "            x = self.np_random.randint(low=150, high=200)\n",
    "            y = self.np_random.randint(low=25, high=190)\n",
    "            vx = - self.np_random.randint(low=10, high=25)\n",
    "            vy = - self.np_random.randint(low=0, high=5)\n",
    "            obj = np.array([x, y, vx, vy])\n",
    "            state = np.append(state, obj)\n",
    "            driver = BasicDriverModel(statio=stationarity)\n",
    "            self.drivers.append(driver)\n",
    "            \n",
    "        print(state)  \n",
    "        self.s = state\n",
    "        \n",
    "        return self.s\n",
    "    \n",
    "    # TODO reward shaping and reward basic\n",
    "    def _reward(self, s, a, sp):\n",
    "        # Keep track for visualization, plots ...\n",
    "        self.dist_to_goal = get_dist_to_goal(sp, self.goal)\n",
    "        self.dist_nearest_obj = get_dist_nearest_obj(sp)\n",
    "        self.smallest_TTC, self.smallest_TTC_obj = get_smallest_TTC(sp)\n",
    "\n",
    "        # We are dealiong with 3 types of objectives:\n",
    "        # - COMFORT (weiht 1)\n",
    "        # - EFFICIENCY (weight 10)\n",
    "        # - SAFETY (weight 100)\n",
    "\n",
    "        r_comfort = r_efficiency = r_safety = 0\n",
    "\n",
    "        if self.reward_shaping and self.smallest_TTC <= 10.0:\n",
    "            r_safety += -10 - (10 - self.smallest_TTC) * 10 # between [-100, -10]\n",
    "\n",
    "        # SAFETY related + terminal state (overwrite)\n",
    "        if self.dist_nearest_obj <= self.dist_collision:\n",
    "            r_safety += -1000\n",
    "\n",
    "        # The faster we go in this test setup\n",
    "        r_efficiency += a\n",
    "\n",
    "        if a < -2:\n",
    "            r_comfort += -1\n",
    "\n",
    "        # Keep track for visualization, plots ...\n",
    "        self.r_comfort = r_comfort\n",
    "        self.r_efficiency = r_efficiency\n",
    "        self.r_safety = r_safety\n",
    "\n",
    "        return r_comfort + r_efficiency + r_safety\n",
    "        \n",
    "    def render(self):\n",
    "        pos_left = 40\n",
    "        #color_text = (255,255,255)\n",
    "        color_text = (0,0,0)\n",
    "        img = np.zeros([250, 250, 3],dtype=np.uint8)\n",
    "        img.fill(255) # or img[:] = 255\n",
    "        cv2.putText(img, 'Anti Collision Tests', (pos_left, 240), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255))\n",
    "        \n",
    "        x = self.s[0]; y = self.s[1]; vx = self.s[2]; vy = self.s[3]; v = int(math.sqrt(vx**2 + vy**2)*3.6)\n",
    "        color = (0, 0, 255) # blue\n",
    "        cv2.circle(img, (x, y), 2, color, -1)\n",
    "        draw_arrow(img, (int(x), int(y)), (int(x+vx), int(y+vy)), color)        \n",
    "        cv2.putText(img, str(v) + ' kmh', (x+vx+5, y+vy), cv2.FONT_HERSHEY_SIMPLEX, 0.25, color)\n",
    "        \n",
    "        for i in range(self.nobjs):\n",
    "            if i == self.smallest_TTC_obj:\n",
    "                color = (255, 0, 0) # red\n",
    "            else:\n",
    "                color = (0, 2500, 0) # green\n",
    "            idx = (i+1)*4\n",
    "            x = self.s[idx]; y = self.s[idx+1]; vx = self.s[idx+2]; vy = self.s[idx+3]; v = int(math.sqrt(vx**2 + vy**2)*3.6)\n",
    "            cv2.circle(img, (x, y), 2, color, -1)\n",
    "            draw_arrow(img, (int(x), int(y)), (int(x+vx), int(y+vy)), color)        \n",
    "            cv2.putText(img, str(v) + ' kmh', (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.25, color_text)\n",
    "        \n",
    "        if self.reward is not None:\n",
    "            str_reward = \"R_com %.2f , R_eff %.2f R_saf %.2f\" % (self.r_comfort, self.r_efficiency, self.r_safety)\n",
    "            cv2.putText(img, str_reward, (pos_left, 205), cv2.FONT_HERSHEY_SIMPLEX, 0.25, color_text)\n",
    "        \n",
    "            str_safety = \"TTC %.2f seconds, D_min %.2f meters\" % (self.smallest_TTC, self.dist_nearest_obj)\n",
    "            cv2.putText(img, str_safety, (pos_left, 215), cv2.FONT_HERSHEY_SIMPLEX, 0.25, color_text)\n",
    "            \n",
    "            str_step = \"Step %d with action %d reward %.2f\" % (self.steps, self.action, self.reward)\n",
    "            cv2.putText(img, str_step, (pos_left, 225), cv2.FONT_HERSHEY_SIMPLEX, 0.25, (0,0,255))\n",
    "        \n",
    "        #img = cv2.resize(img, None, fx=20, fy=20)\n",
    "        #img = cv2.resize(img,(2500, 2500))\n",
    "        return img\n",
    "        \n",
    "    #state, reward, done, info = env.step(action)\n",
    "    def step(self, action):\n",
    "        reward = -1; done = False; info = {}        \n",
    "        sp = copy.copy(self.s)\n",
    "        \n",
    "        s = self.s[0:4]\n",
    "        a = np.array([0.0, action])\n",
    "        sp[0:4] = transition_ca(s, a)\n",
    "        \n",
    "        idx = 4\n",
    "        for n in range(self.nobjs):\n",
    "            s_obj = self.s[idx:idx+4]\n",
    "            accel, state = self.drivers[n].step() # CALL driver model\n",
    "            #print(\"OBJ {} accel {} state {}\".format(n, accel, state))\n",
    "            a_obj = np.array([accel, 0.0])\n",
    "            sp[idx:idx+4] = transition_ca(s_obj, a_obj)\n",
    "            idx += 4\n",
    "            \n",
    "        reward = self._reward(self.s, action, sp)\n",
    "        \n",
    "        self.s = sp\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.steps += 1\n",
    "        \n",
    "        if self.dist_nearest_obj <= self.dist_collision or self.s[1] >= self.goal[1]:\n",
    "            print(\"done: dist_nearest_obj {}, y-ego {}\".format(self.dist_nearest_obj, self.s[1]))\n",
    "            done = True\n",
    "            if self.steps_beyond_done > 0:\n",
    "                logger.warn(\"You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\")\n",
    "            self.steps_beyond_done += 1\n",
    "            if self.dist_nearest_obj <= self.dist_collision:\n",
    "                info = \"fail\"\n",
    "            else:\n",
    "                info = \"success\"\n",
    "                \n",
    "        return self.s, reward, done, info\n",
    "    \n",
    "    def close(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED 8689754769861327084\n",
      "[100   0   0  20  21 136  20   2  40 102  21   3  46  70  10   0   4 167\n",
      "  16   0   3 175  24   1 182 172 -23  -4 166 132 -23  -3 170  83 -23  -3\n",
      " 197  33 -12  -4 197 104 -12  -3]\n"
     ]
    }
   ],
   "source": [
    "env = ActEnv(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAksAAAJCCAYAAADQsoPKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAEBDWAIAaPx/DEOpckK1NlIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = env.render()\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Start episode 0\n",
      "[100   0   0  20  23  53  15   0  38 115  14   4  47 112  22   1   0  93\n",
      "  16   1   1  49  17   1 198 133 -16   0 169 108 -13  -2 173  38 -16  -3\n",
      " 167  27 -14  -2 164 105 -23  -2]\n",
      "Step 0: action=0.0 reward=-67.76834796534487 done=False\n",
      "Step 1: action=0.0 reward=-70.05482605369218 done=False\n",
      "Step 2: action=0.0 reward=-72.34125846721363 done=False\n",
      "Step 3: action=0.0 reward=-74.6276453193682 done=False\n",
      "Step 4: action=0.0 reward=-76.91398672213455 done=False\n",
      "Step 5: action=0.0 reward=-79.20028278602635 done=False\n",
      "Step 6: action=0.0 reward=-81.48653362010678 done=False\n",
      "Step 7: action=0.0 reward=-83.77273933200345 done=False\n",
      "Step 8: action=0.0 reward=-86.05890002792243 done=False\n",
      "Step 9: action=0.0 reward=-88.34501581266244 done=False\n",
      "Step 10: action=0.0 reward=-90.63108678962857 done=False\n",
      "Step 11: action=0.0 reward=-92.91711306084588 done=False\n",
      "Step 12: action=0.0 reward=-95.20309472697264 done=False\n",
      "Step 13: action=0.0 reward=-97.48903188731344 done=False\n",
      "Step 14: action=0.0 reward=-99.77492463983197 done=False\n",
      "Step 15: action=0.0 reward=-102.06077308116362 done=False\n",
      "Step 16: action=0.0 reward=-104.34657730662786 done=False\n",
      "Step 17: action=0.0 reward=-106.63233741024041 done=False\n",
      "Step 18: action=0.0 reward=-108.91805348472501 done=False\n",
      "Step 19: action=0.0 reward=-99.82230806913316 done=False\n",
      "done: dist_nearest_obj 6.708203932499369, y-ego 84\n",
      "Step 20: action=0.0 reward=-1102.1002326129203 done=True\n",
      "End of episode 0 with cumulated_reward -2880.465069175878\n",
      "====> Start episode 1\n",
      "[100   0   0  20  30 139  18   4  29  57  15   2  37  84  11   0  26 178\n",
      "  22   1  43  76  21   1 174 172 -18  -4 169 129 -16  -2 176 172 -23  -1\n",
      " 186  28 -18  -2 171 135 -21  -4]\n",
      "Step 0: action=0.0 reward=-67.18447771096915 done=False\n",
      "Step 1: action=0.0 reward=0.0 done=False\n",
      "Step 2: action=0.0 reward=0.0 done=False\n",
      "Step 3: action=0.0 reward=0.0 done=False\n",
      "Step 4: action=0.0 reward=0.0 done=False\n",
      "Step 5: action=0.0 reward=0.0 done=False\n",
      "Step 6: action=0.0 reward=0.0 done=False\n",
      "Step 7: action=0.0 reward=0.0 done=False\n",
      "Step 8: action=0.0 reward=0.0 done=False\n",
      "Step 9: action=0.0 reward=0.0 done=False\n",
      "Step 10: action=0.0 reward=0.0 done=False\n",
      "Step 11: action=0.0 reward=-102.01995012468828 done=False\n",
      "Step 12: action=0.0 reward=-104.43586363398525 done=False\n",
      "Step 13: action=0.0 reward=-106.74781659303622 done=False\n",
      "Step 14: action=0.0 reward=-109.00558922449527 done=False\n",
      "Step 15: action=0.0 reward=-106.75170535274111 done=False\n",
      "Step 16: action=0.0 reward=-108.54417078492453 done=False\n",
      "Step 17: action=0.0 reward=0.0 done=False\n",
      "Step 18: action=0.0 reward=0.0 done=False\n",
      "Step 19: action=0.0 reward=0.0 done=False\n",
      "Step 20: action=0.0 reward=0.0 done=False\n",
      "Step 21: action=0.0 reward=0.0 done=False\n",
      "Step 22: action=0.0 reward=0.0 done=False\n",
      "Step 23: action=0.0 reward=0.0 done=False\n",
      "Step 24: action=0.0 reward=0.0 done=False\n",
      "Step 25: action=0.0 reward=0.0 done=False\n",
      "Step 26: action=0.0 reward=0.0 done=False\n",
      "Step 27: action=0.0 reward=0.0 done=False\n",
      "Step 28: action=0.0 reward=0.0 done=False\n",
      "Step 29: action=0.0 reward=0.0 done=False\n",
      "Step 30: action=0.0 reward=-105.77289529092205 done=False\n",
      "Step 31: action=0.0 reward=-109.22838690902068 done=False\n",
      "Step 32: action=0.0 reward=-102.35504244572473 done=False\n",
      "Step 33: action=0.0 reward=-103.41985756584558 done=False\n",
      "Step 34: action=0.0 reward=-105.0 done=False\n",
      "Step 35: action=0.0 reward=-106.96233332127376 done=False\n",
      "Step 36: action=0.0 reward=-109.25122560800902 done=False\n",
      "Step 37: action=0.0 reward=0.0 done=False\n",
      "Step 38: action=0.0 reward=0.0 done=False\n",
      "Step 39: action=0.0 reward=0.0 done=False\n",
      "Step 40: action=0.0 reward=0.0 done=False\n",
      "Step 41: action=0.0 reward=0.0 done=False\n",
      "Step 42: action=0.0 reward=0.0 done=False\n",
      "Step 43: action=0.0 reward=0.0 done=False\n",
      "Step 44: action=0.0 reward=0.0 done=False\n",
      "Step 45: action=0.0 reward=0.0 done=False\n",
      "Step 46: action=0.0 reward=0.0 done=False\n",
      "Step 47: action=0.0 reward=0.0 done=False\n",
      "Step 48: action=0.0 reward=0.0 done=False\n",
      "done: dist_nearest_obj 61.66036003787198, y-ego 200\n",
      "Step 49: action=0.0 reward=0.0 done=True\n",
      "End of episode 1 with cumulated_reward -1446.6793145656357\n",
      "====> Start episode 2\n",
      "[100   0   0  20  32  68  18   4  16  42  21   1   2 115  11   1  44  55\n",
      "  17   2  38  62  15   3 188 175 -11   0 158  54 -21  -4 178  67 -21   0\n",
      " 181  25 -20  -3 176  37 -18   0]\n",
      "Step 0: action=0.0 reward=-91.54413798546062 done=False\n",
      "Step 1: action=0.0 reward=-93.83996727910045 done=False\n",
      "Step 2: action=0.0 reward=-96.12898366904419 done=False\n",
      "Step 3: action=0.0 reward=-98.41155294515738 done=False\n",
      "Step 4: action=0.0 reward=-100.68798745975289 done=False\n",
      "Step 5: action=0.0 reward=-102.9585548661798 done=False\n",
      "Step 6: action=0.0 reward=-105.22348494820561 done=False\n",
      "Step 7: action=0.0 reward=-107.48297501057975 done=False\n",
      "Step 8: action=0.0 reward=-109.73719416737657 done=False\n",
      "done: dist_nearest_obj 8.94427190999916, y-ego 40\n",
      "Step 9: action=0.0 reward=-1103.3479637197079 done=True\n",
      "End of episode 2 with cumulated_reward -2009.362802050565\n",
      "====> Start episode 3\n",
      "[100   0   0  20  28 147  16   3  19 126  24   0  18 108  16   4   7 104\n",
      "  18   0  46  99  11   3 188 177 -18  -2 185  83 -10  -2 178  40 -11  -3\n",
      " 190 148 -16  -3 159 123 -11  -2]\n",
      "Step 0: action=0.0 reward=-65.57470609685375 done=False\n",
      "Step 1: action=0.0 reward=-67.42307032287765 done=False\n",
      "Step 2: action=0.0 reward=-69.26649899365731 done=False\n",
      "Step 3: action=0.0 reward=-71.10497237569061 done=False\n",
      "Step 4: action=0.0 reward=-72.93845728602095 done=False\n",
      "Step 5: action=0.0 reward=-74.76690664184395 done=False\n",
      "Step 6: action=0.0 reward=-76.81565771217291 done=False\n",
      "Step 7: action=0.0 reward=-79.09204999775257 done=False\n",
      "Step 8: action=0.0 reward=-81.35990932877388 done=False\n",
      "Step 9: action=0.0 reward=-83.63636363636363 done=False\n",
      "Step 10: action=0.0 reward=-85.90523744472134 done=False\n",
      "Step 11: action=0.0 reward=-88.16612083447308 done=False\n",
      "Step 12: action=0.0 reward=-90.41853516293997 done=False\n",
      "Step 13: action=0.0 reward=-92.66191816766737 done=False\n",
      "Step 14: action=0.0 reward=-94.89560437012229 done=False\n",
      "Step 15: action=0.0 reward=-97.11879884749891 done=False\n",
      "Step 16: action=0.0 reward=-99.33054141860806 done=False\n",
      "Step 17: action=0.0 reward=-101.52965658753953 done=False\n",
      "Step 18: action=0.0 reward=-103.71468164140364 done=False\n",
      "Step 19: action=0.0 reward=-105.88375995909053 done=False\n",
      "Step 20: action=0.0 reward=-108.03447638189402 done=False\n",
      "Step 21: action=0.0 reward=-105.2602083299999 done=False\n",
      "Step 22: action=0.0 reward=-107.5 done=False\n",
      "done: dist_nearest_obj 6.708203932499369, y-ego 96\n",
      "Step 23: action=0.0 reward=-1109.7245975256071 done=True\n",
      "End of episode 3 with cumulated_reward -3132.1227290635725\n",
      "====> Start episode 4\n",
      "[100   0   0  20  43 138  16   2  42 147  22   0  10 185  14   4  35  38\n",
      "  20   1  21 112  14   4 176 167 -21  -2 196  30 -23  -3 159 155 -24  -2\n",
      " 166  34 -22  -3 196 127 -19   0]\n",
      "Step 0: action=0.0 reward=-51.297419805857025 done=False\n",
      "Step 1: action=0.0 reward=-54.163016514873384 done=False\n",
      "Step 2: action=0.0 reward=-56.83198783215049 done=False\n",
      "Step 3: action=0.0 reward=-59.374676825810226 done=False\n",
      "Step 4: action=0.0 reward=-61.824531152950684 done=False\n",
      "Step 5: action=0.0 reward=-64.2004930195282 done=False\n",
      "Step 6: action=0.0 reward=-66.51440344482822 done=False\n",
      "Step 7: action=0.0 reward=-68.77411548511039 done=False\n",
      "Step 8: action=0.0 reward=-70.98500950991432 done=False\n",
      "Step 9: action=0.0 reward=-73.15080506227149 done=False\n",
      "Step 10: action=0.0 reward=-75.27402301750283 done=False\n",
      "Step 11: action=0.0 reward=-77.3562554699314 done=False\n",
      "Step 12: action=0.0 reward=-79.39831891661899 done=False\n",
      "Step 13: action=0.0 reward=-81.40032777852964 done=False\n",
      "Step 14: action=0.0 reward=-83.36170443475298 done=False\n",
      "Step 15: action=0.0 reward=-85.28112789399697 done=False\n",
      "Step 16: action=0.0 reward=-87.1564100252395 done=False\n",
      "Step 17: action=0.0 reward=-88.98427052044566 done=False\n",
      "Step 18: action=0.0 reward=-90.75995091865069 done=False\n",
      "Step 19: action=0.0 reward=-92.47654519516908 done=False\n",
      "Step 20: action=0.0 reward=-94.12377980016558 done=False\n",
      "Step 21: action=0.0 reward=-95.68559704683557 done=False\n",
      "Step 22: action=0.0 reward=-97.13472126584347 done=False\n",
      "Step 23: action=0.0 reward=-98.41772633126668 done=False\n",
      "Step 24: action=0.0 reward=-99.39548738037996 done=False\n",
      "Step 25: action=0.0 reward=0.0 done=False\n",
      "Step 26: action=0.0 reward=0.0 done=False\n",
      "Step 27: action=0.0 reward=0.0 done=False\n",
      "Step 28: action=0.0 reward=0.0 done=False\n",
      "Step 29: action=0.0 reward=0.0 done=False\n",
      "Step 30: action=0.0 reward=0.0 done=False\n",
      "Step 31: action=0.0 reward=0.0 done=False\n",
      "Step 32: action=0.0 reward=0.0 done=False\n",
      "Step 33: action=0.0 reward=0.0 done=False\n",
      "Step 34: action=0.0 reward=-89.53170680830706 done=False\n",
      "Step 35: action=0.0 reward=-93.03277736080503 done=False\n",
      "Step 36: action=0.0 reward=-95.96931616897822 done=False\n",
      "Step 37: action=0.0 reward=-98.6784067828041 done=False\n",
      "Step 38: action=0.0 reward=-101.2486784800466 done=False\n",
      "Step 39: action=0.0 reward=-103.71957897117952 done=False\n",
      "Step 40: action=0.0 reward=-106.11269820358143 done=False\n",
      "Step 41: action=0.0 reward=-108.44126311418576 done=False\n",
      "Step 42: action=0.0 reward=-97.60464276561373 done=False\n",
      "done: dist_nearest_obj 9.219544457292887, y-ego 176\n",
      "Step 43: action=0.0 reward=-1099.4527444407997 done=True\n",
      "End of episode 4 with cumulated_reward -3947.1145177449243\n",
      "METRICS: SAFETY %collisions = 0.8, COMFORT MeanHardBrake = 0.0, EFFICIENCY MeanStepsToGoal = 49.0\n"
     ]
    }
   ],
   "source": [
    "max_episodes = 5\n",
    "max_steps = 120\n",
    "\n",
    "# METRICS\n",
    "metric_success = 0 # EFFICIENCY\n",
    "metric_steps_to_goal = [] # SAFETY\n",
    "metric_hardbrake = [] # COMFORT\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    print(\"====> Start episode {}\".format(episode))\n",
    "    env.reset()\n",
    "    cumulated_reward = 0\n",
    "    images = []\n",
    "    \n",
    "    hardbrake = 0    \n",
    "    for n in range(max_steps):\n",
    "        action = 0.0\n",
    "        if action <= -2:\n",
    "            hardbrake += 1\n",
    "        state, reward, done, info = env.step(action)\n",
    "        cumulated_reward += reward\n",
    "        print(\"Step {}: action={} reward={} done={}\".format(n, action, reward, done)) # PHW DEBUG\n",
    "        img = env.render()\n",
    "        images.append(img)\n",
    "        if done is True:\n",
    "            if info == \"success\":\n",
    "                metric_success += 1\n",
    "                metric_steps_to_goal.append(n)\n",
    "                metric_hardbrake.append(hardbrake)\n",
    "            print(\"End of episode {} with cumulated_reward {}\".format(episode, cumulated_reward))\n",
    "            break\n",
    "            \n",
    "print(\"METRICS: SAFETY %collisions = {}, COMFORT MeanHardBrake = {}, EFFICIENCY MeanStepsToGoal = {}\".format(1-metric_success/max_episodes, np.mean(metric_hardbrake), np.mean(metric_steps_to_goal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "1\n",
      "[49]\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(metric_hardbrake)\n",
    "print(metric_success)\n",
    "print(metric_steps_to_goal)\n",
    "print(max_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAksAAAJCCAYAAADQsoPKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_img(images[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"img/visu.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imageio.mimsave('img/visu.gif', images, duration=0.2)\n",
    "HTML(\"\"\"<img src=\"img/visu.gif\"/>\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"img/visu2.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_images = resize_images(images, f=2)\n",
    "imageio.mimsave('img/visu2.gif', big_images, duration=0.2)\n",
    "HTML(\"\"\"<img src=\"img/visu2.gif\"/>\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
