{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of https://github.com/openai/spinningup/blob/master/spinup/algos/ddpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls spinup/algos/ddpg/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logx.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls spinup/utils/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ddpg.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spinup.algos.ddpg import core\n",
    "from spinup.algos.ddpg.core import get_vars\n",
    "from spinup.utils.logx import EpochLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    A simple FIFO experience replay buffer for DDPG agents.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, size):\n",
    "        self.obs1_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.obs2_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.acts_buf = np.zeros([size, act_dim], dtype=np.float32)\n",
    "        self.rews_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ptr, self.size, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, next_obs, done):\n",
    "        self.obs1_buf[self.ptr] = obs\n",
    "        self.obs2_buf[self.ptr] = next_obs\n",
    "        self.acts_buf[self.ptr] = act\n",
    "        self.rews_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr+1) % self.max_size\n",
    "        self.size = min(self.size+1, self.max_size)\n",
    "\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "        return dict(obs1=self.obs1_buf[idxs],\n",
    "                    obs2=self.obs2_buf[idxs],\n",
    "                    acts=self.acts_buf[idxs],\n",
    "                    rews=self.rews_buf[idxs],\n",
    "                    done=self.done_buf[idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Deep Deterministic Policy Gradient (DDPG)\n",
    "\"\"\"\n",
    "def ddpg(env_fn, actor_critic=core.mlp_actor_critic, ac_kwargs=dict(), seed=0, \n",
    "         steps_per_epoch=5000, epochs=100, replay_size=int(1e6), gamma=0.99, \n",
    "         polyak=0.995, pi_lr=1e-3, q_lr=1e-3, batch_size=100, start_steps=10000, \n",
    "         act_noise=0.1, max_ep_len=1000, logger_kwargs=dict(), save_freq=1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        env_fn : A function which creates a copy of the environment.\n",
    "            The environment must satisfy the OpenAI Gym API.\n",
    "        actor_critic: A function which takes in placeholder symbols \n",
    "            for state, ``x_ph``, and action, ``a_ph``, and returns the main \n",
    "            outputs from the agent's Tensorflow computation graph:\n",
    "            ===========  ================  ======================================\n",
    "            Symbol       Shape             Description\n",
    "            ===========  ================  ======================================\n",
    "            ``pi``       (batch, act_dim)  | Deterministically computes actions\n",
    "                                           | from policy given states.\n",
    "            ``q``        (batch,)          | Gives the current estimate of Q* for \n",
    "                                           | states in ``x_ph`` and actions in\n",
    "                                           | ``a_ph``.\n",
    "            ``q_pi``     (batch,)          | Gives the composition of ``q`` and \n",
    "                                           | ``pi`` for states in ``x_ph``: \n",
    "                                           | q(x, pi(x)).\n",
    "            ===========  ================  ======================================\n",
    "        ac_kwargs (dict): Any kwargs appropriate for the actor_critic \n",
    "            function you provided to DDPG.\n",
    "        seed (int): Seed for random number generators.\n",
    "        steps_per_epoch (int): Number of steps of interaction (state-action pairs) \n",
    "            for the agent and the environment in each epoch.\n",
    "        epochs (int): Number of epochs to run and train agent.\n",
    "        replay_size (int): Maximum length of replay buffer.\n",
    "        gamma (float): Discount factor. (Always between 0 and 1.)\n",
    "        polyak (float): Interpolation factor in polyak averaging for target \n",
    "            networks. Target networks are updated towards main networks \n",
    "            according to:\n",
    "            .. math:: \\\\theta_{\\\\text{targ}} \\\\leftarrow \n",
    "                \\\\rho \\\\theta_{\\\\text{targ}} + (1-\\\\rho) \\\\theta\n",
    "            where :math:`\\\\rho` is polyak. (Always between 0 and 1, usually \n",
    "            close to 1.)\n",
    "        pi_lr (float): Learning rate for policy.\n",
    "        q_lr (float): Learning rate for Q-networks.\n",
    "        batch_size (int): Minibatch size for SGD.\n",
    "        start_steps (int): Number of steps for uniform-random action selection,\n",
    "            before running real policy. Helps exploration.\n",
    "        act_noise (float): Stddev for Gaussian exploration noise added to \n",
    "            policy at training time. (At test time, no noise is added.)\n",
    "        max_ep_len (int): Maximum length of trajectory / episode / rollout.\n",
    "        logger_kwargs (dict): Keyword args for EpochLogger.\n",
    "        save_freq (int): How often (in terms of gap between epochs) to save\n",
    "            the current policy and value function.\n",
    "    \"\"\"\n",
    "\n",
    "    logger = EpochLogger(**logger_kwargs)\n",
    "    logger.save_config(locals())\n",
    "\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    env, test_env = env_fn(), env_fn()\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "\n",
    "    # Action limit for clamping: critically, assumes all dimensions share the same bound!\n",
    "    act_limit = env.action_space.high[0]\n",
    "\n",
    "    # Share information about action space with policy architecture\n",
    "    ac_kwargs['action_space'] = env.action_space\n",
    "\n",
    "    # Inputs to computation graph\n",
    "    x_ph, a_ph, x2_ph, r_ph, d_ph = core.placeholders(obs_dim, act_dim, obs_dim, None, None)\n",
    "\n",
    "    # Main outputs from computation graph\n",
    "    with tf.variable_scope('main'):\n",
    "        pi, q, q_pi = actor_critic(x_ph, a_ph, **ac_kwargs)\n",
    "    \n",
    "    # Target networks\n",
    "    with tf.variable_scope('target'):\n",
    "        # Note that the action placeholder going to actor_critic here is \n",
    "        # irrelevant, because we only need q_targ(s, pi_targ(s)).\n",
    "        pi_targ, _, q_pi_targ  = actor_critic(x2_ph, a_ph, **ac_kwargs)\n",
    "\n",
    "    # Experience buffer\n",
    "    replay_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=replay_size)\n",
    "\n",
    "    # Count variables\n",
    "    var_counts = tuple(core.count_vars(scope) for scope in ['main/pi', 'main/q', 'main'])\n",
    "    print('\\nNumber of parameters: \\t pi: %d, \\t q: %d, \\t total: %d\\n'%var_counts)\n",
    "\n",
    "    # Bellman backup for Q function\n",
    "    backup = tf.stop_gradient(r_ph + gamma*(1-d_ph)*q_pi_targ)\n",
    "\n",
    "    # DDPG losses\n",
    "    pi_loss = -tf.reduce_mean(q_pi)\n",
    "    q_loss = tf.reduce_mean((q-backup)**2)\n",
    "\n",
    "    # Separate train ops for pi, q\n",
    "    pi_optimizer = tf.train.AdamOptimizer(learning_rate=pi_lr)\n",
    "    q_optimizer = tf.train.AdamOptimizer(learning_rate=q_lr)\n",
    "    train_pi_op = pi_optimizer.minimize(pi_loss, var_list=get_vars('main/pi'))\n",
    "    train_q_op = q_optimizer.minimize(q_loss, var_list=get_vars('main/q'))\n",
    "\n",
    "    # Polyak averaging for target variables\n",
    "    target_update = tf.group([tf.assign(v_targ, polyak*v_targ + (1-polyak)*v_main)\n",
    "                              for v_main, v_targ in zip(get_vars('main'), get_vars('target'))])\n",
    "\n",
    "    # Initializing targets to match main variables\n",
    "    target_init = tf.group([tf.assign(v_targ, v_main)\n",
    "                              for v_main, v_targ in zip(get_vars('main'), get_vars('target'))])\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(target_init)\n",
    "\n",
    "    # Setup model saving\n",
    "    logger.setup_tf_saver(sess, inputs={'x': x_ph, 'a': a_ph}, outputs={'pi': pi, 'q': q})\n",
    "\n",
    "    def get_action(o, noise_scale):\n",
    "        a = sess.run(pi, feed_dict={x_ph: o.reshape(1,-1)})[0]\n",
    "        a += noise_scale * np.random.randn(act_dim)\n",
    "        return np.clip(a, -act_limit, act_limit)\n",
    "\n",
    "    def test_agent(n=10):\n",
    "        for j in range(n):\n",
    "            o, r, d, ep_ret, ep_len = test_env.reset(), 0, False, 0, 0\n",
    "            while not(d or (ep_len == max_ep_len)):\n",
    "                # Take deterministic actions at test time (noise_scale=0)\n",
    "                o, r, d, _ = test_env.step(get_action(o, 0))\n",
    "                ep_ret += r\n",
    "                ep_len += 1\n",
    "            logger.store(TestEpRet=ep_ret, TestEpLen=ep_len)\n",
    "\n",
    "    start_time = time.time()\n",
    "    o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0\n",
    "    total_steps = steps_per_epoch * epochs\n",
    "\n",
    "    # Main loop: collect experience in env and update/log each epoch\n",
    "    for t in range(total_steps):\n",
    "\n",
    "        \"\"\"\n",
    "        Until start_steps have elapsed, randomly sample actions\n",
    "        from a uniform distribution for better exploration. Afterwards, \n",
    "        use the learned policy (with some noise, via act_noise). \n",
    "        \"\"\"\n",
    "        if t > start_steps:\n",
    "            a = get_action(o, act_noise)\n",
    "        else:\n",
    "            a = env.action_space.sample()\n",
    "\n",
    "        # Step the env\n",
    "        o2, r, d, _ = env.step(a)\n",
    "        ep_ret += r\n",
    "        ep_len += 1\n",
    "\n",
    "        # Ignore the \"done\" signal if it comes from hitting the time\n",
    "        # horizon (that is, when it's an artificial terminal signal\n",
    "        # that isn't based on the agent's state)\n",
    "        d = False if ep_len==max_ep_len else d\n",
    "\n",
    "        # Store experience to replay buffer\n",
    "        replay_buffer.store(o, a, r, o2, d)\n",
    "\n",
    "        # Super critical, easy to overlook step: make sure to update \n",
    "        # most recent observation!\n",
    "        o = o2\n",
    "\n",
    "        if d or (ep_len == max_ep_len):\n",
    "            \"\"\"\n",
    "            Perform all DDPG updates at the end of the trajectory,\n",
    "            in accordance with tuning done by TD3 paper authors.\n",
    "            \"\"\"\n",
    "            for _ in range(ep_len):\n",
    "                batch = replay_buffer.sample_batch(batch_size)\n",
    "                feed_dict = {x_ph: batch['obs1'],\n",
    "                             x2_ph: batch['obs2'],\n",
    "                             a_ph: batch['acts'],\n",
    "                             r_ph: batch['rews'],\n",
    "                             d_ph: batch['done']\n",
    "                            }\n",
    "\n",
    "                # Q-learning update\n",
    "                outs = sess.run([q_loss, q, train_q_op], feed_dict)\n",
    "                logger.store(LossQ=outs[0], QVals=outs[1])\n",
    "\n",
    "                # Policy update\n",
    "                outs = sess.run([pi_loss, train_pi_op, target_update], feed_dict)\n",
    "                logger.store(LossPi=outs[0])\n",
    "\n",
    "            logger.store(EpRet=ep_ret, EpLen=ep_len)\n",
    "            o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0\n",
    "\n",
    "        # End of epoch wrap-up\n",
    "        if t > 0 and t % steps_per_epoch == 0:\n",
    "            epoch = t // steps_per_epoch\n",
    "\n",
    "            # Save model\n",
    "            if (epoch % save_freq == 0) or (epoch == epochs-1):\n",
    "                logger.save_state({'env': env}, None)\n",
    "\n",
    "            # Test the performance of the deterministic version of the agent.\n",
    "            test_agent()\n",
    "\n",
    "            # Log info about epoch\n",
    "            logger.log_tabular('Epoch', epoch)\n",
    "            logger.log_tabular('EpRet', with_min_and_max=True)\n",
    "            logger.log_tabular('TestEpRet', with_min_and_max=True)\n",
    "            logger.log_tabular('EpLen', average_only=True)\n",
    "            logger.log_tabular('TestEpLen', average_only=True)\n",
    "            logger.log_tabular('TotalEnvInteracts', t)\n",
    "            logger.log_tabular('QVals', with_min_and_max=True)\n",
    "            logger.log_tabular('LossPi', average_only=True)\n",
    "            logger.log_tabular('LossQ', average_only=True)\n",
    "            logger.log_tabular('Time', time.time()-start_time)\n",
    "            logger.dump_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env='HalfCheetah-v1'\n",
    "env='InvertedPendulum-v1'\n",
    "env='Act-v0'\n",
    "hid=300\n",
    "l=1\n",
    "gamma=0.99\n",
    "seed=0\n",
    "epochs=10\n",
    "exp_name='ddpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spinup.utils.run_utils import setup_logger_kwargs\n",
    "logger_kwargs = setup_logger_kwargs(exp_name, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg(lambda : gym.make(env), \n",
    "     actor_critic=core.mlp_actor_critic,\n",
    "     ac_kwargs=dict(hidden_sizes=[hid]*l),\n",
    "     gamma=gamma, \n",
    "     seed=seed, \n",
    "     epochs=epochs, \n",
    "     logger_kwargs=logger_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
