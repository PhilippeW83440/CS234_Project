%% LyX 2.3.0 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{babel}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{stmaryrd}
\usepackage{microtype}
\usepackage[unicode=true]
 {hyperref}
\usepackage{breakurl}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage[final]{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2016}

%\usepackage[utf8]{inputenc} % allow utf-8 input
%\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{smartdiagram}

\makeatother

\usepackage[style=numeric,backend=bibtex8]{biblatex}
\addbibresource{safeRL.bib}
\begin{document}

\title{Reinforcement Learning with Hard Constraints for Autonomous Driving:
CS234 project proposal}

\author{Philippe Weingertner, Vaishali G Kulkarni\\
 \texttt{pweinger@stanford.edu,} \texttt{vaishali@stanford.edu}\\
}
\maketitle

\section{Problem Definition}

Reinforcement Learning (RL) has demonstrated its capability to learn
efficient strategies on many different and complex tasks. In particular,
in games like chess and go, the best human players have lost against
RL algorithms (\textcite{silver2017mastering}). There is a growing
traction for applying such RL algorithms to complex robotics tasks
like Autonomous Driving. Nevertheless with Autonomous Driving we are
dealing with additional challenges. We are in a partially observable
environment where enforcing safety is of paramount importance. As
a consequence, considering safety via a reward and the optimization
of a statistical criteria is not sufficient. Hard Constraints have
to be enforced all the time. We propose to study how the RL optimization
criteria can be modified to deal with hard constraints; how algorithms
like DQN could be modified to cope with such hard constraints and
more generally how an RL agent could be integrated in a Decision Making
module for Autonomous Driving to provide efficient and scalable strategies
while still providing safety guarantees. So we propose to address
the following problem formulation:

\[
\underset{\theta}{max}\:\mathbb{E}[\sum_{t=0}^{\infty}\gamma^{t}R(s_{t},\pi_{\theta}(s_{t}))]
\]

\[
\text{ s.t. }c_{i}(s_{t})\leq C_{i}\:\forall i\in\left\llbracket 1,K\right\rrbracket 
\]

where the expectation corresponds to the statistical RL objective
and $c_{i}$ correspond to hard safety constraints.

\section{Problem Simulation}

We will upgrade the Anti Collision Tests environment, \href{https://github.com/PhilippeW83440/ACT}{ACT},
developed for a previous CS238 Stanford project. A vehicle has to
drive from a point A to a point B as fast as possible, while avoiding
other vehicles that are crossing its path and trying to minimize hard
braking decisions. So it is a multi objectives task where efficiency,
comfort and safety objectives have to be optimized. While in the previous
project we studied the applicability of POMDPs solvers for decision
making in a context of sensors uncertainty, we will deal here with
an even more challenging task: the uncertainty will be related to
the other vehicles driving models. Initially other vehicles driving
models were simple Constant Velocity models. Here we will use Intelligent
Driver Models, \href{https://en.wikipedia.org/wiki/Intelligent_driver_model}{IDM},
depending on 5 parameters that will be unknown to the ego vehicle,
and that will differ per vehicle. So it is a model-free setting: we
do not know the model of the environment, the driving models of others,
and we would like to learn to drive efficiently and safely in this
context.

\section{Proposed Algorithms}

A DQN network will be used as a baseline (we may change later to Policy
Gradients or Actor Critic. This is a topic for further refinement).
We will consider 3 type of modifications. From the most simple, conceptually,
to the most complex, we will:
\begin{enumerate}
\item Propose a post DQN safety check. While the DQN network will compute
$Q(s,a_{i})$ values for every possible actions, we want to exclude
the actions that are unsafe, before deciding what action to take (before
taking the $argmax_{a_{i}}\;Q(s,a_{i})$). This type of approach is
used in a paper from BMW Group by \textcite{inproceedings}.
\item Modify the DQN training algorithm and especially the exploration process
so that only safe actions are explored. Similar to \textcite{Bouton2018uai},
the idea is to derive an exploration strategy that constrains the
agent to choose among actions that satisfy safety criteria. Hence
the search space of policies is restricted to a ``safe'' or safer
subspace of policies.
\item Replace the RL objective $\underset{\theta}{max}\:\mathbb{E}[\sum_{t=0}^{\infty}\gamma^{t}R(s_{t},\pi_{\theta}(s_{t}))]$
by an objective taking into account hard constraints 
\[
\underset{\theta}{max}\:\mathbb{E}[\sum_{t=0}^{\infty}\gamma^{t}R(s_{t},\pi_{\theta}(s_{t}))]
\]
\[
\text{ s.t. }c_{i}(s_{t})\leq C_{i}\:\forall i\in\left\llbracket 1,K\right\rrbracket 
\]
 and study how RL algorithms like DQN should be modified to account
for this new objective. In a recent paper from DeepMind from \textcite{bohez2019success},
this type of approach is applied to a realistic, energy-optimized
robotic locomotion task, using the Minitaur quadruped developed by
\href{https://www.ghostrobotics.io/}{ Ghost Robotics}.
\end{enumerate}

\section{Related Work}

In \textcite{inproceedings} a DQN network is used for tactical decision
making in an autonomous driving pipeline but the DQN algorithm itself
is not modified to handle hard constraints and the safety is guaranteed
by checking the output of the RL algorithm. Our objective here, in
contrast, would be to have an RL algorithm that is directly dealing
with hard constraints to avoid frequent and sub-optimal actions masking.
A review of the different safe RL techniques has been done in \textcite{JMLR:v16:garcia15a}.
Some techniques mainly deal with soft constraints by either reshaping
the reward or trying to minimize the variance related to the risk
of making unsafe decisions, while other try to handle hard constraints.
Garcia et al. have analyzed and categorized safe RL techniques in
two families of approaches: one consists in modifying the exploration
process while the other consists in modifying the optimality criterion.
In \textcite{Leurent2018ApproximateRC} the RL objective is replaced
by a surrogate objective which captures hard constraints and handles
model uncertainty by defining a lower bound of the expectation objective.
In \textcite{DBLP:journals/corr/AchiamHTA17} constrained policy optimization
is solved with a modified trust-region policy gradient. The algorithm's
update rule projects the policy to a safe feasibility set in each
iteration. But the policy is kept within constraints only in expectation.
In \textcite{DBLP:journals/corr/abs-1801-08757} they directly add
to the policy a safety layer that analytically solves an action correction
formulation per each state. This safety layer is learned beforehand
but is approximated by a first order linear approximator. In \textcite{DBLP:journals/corr/abs-1805-11074}
and in \textcite{bohez2019success} the proposed approaches are completely
in line with our objective here: modifying the RL objective such that
it deals directly with hard constraints. But there is no closed form
solution for such a problem and a Lagrangian relaxation technique
is used for solving the constrained optimization problem. Given a
Constrained Markov Decision Process (CMDP), the unconstrained problem
is transformed to $\underset{\lambda\geq0}{min}\;\underset{\theta}{max}\;L(\lambda,\theta)=\underset{\lambda\geq0}{min}\;\underset{\theta}{max}\;[J_{R}^{\pi_{\theta}}-\lambda(J_{C}^{\pi_{\theta}}-\alpha)]$
where $L$ is the Lagrangian and $\lambda$ the Lagrange multiplier
(a penalty coefficient). We propose to study how such techniques could
be applied to the Decision Making process of an Autonomous Driving
pipeline and we will benchmark different RL algorithms, modified to
cope with hard constraints, in an Anti Collision Tests setting.

\section{Evaluation Metrics}

In order to measure success and benchmark different versions of the
algorithms, we will use 3 metrics: a safety metric (percentage of
collisions), an efficiency metric (time to goal), and a comfort metric
(number of hard braking decisions or jerk). We want to enforce safety
while not compromising too much efficiency or comfort: a safe AD vehicle
that would use many regular hard braking decisions would not be acceptable
and could even be dangerous for other vehicles.

\nocite{*}
\printbibliography

\end{document}
