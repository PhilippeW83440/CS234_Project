#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage[final]{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2016}

%\usepackage[utf8]{inputenc} % allow utf-8 input
%\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{smartdiagram}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package babel
\inputencoding utf8
\fontencoding T1
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype true
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement H
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine biblatex
\cite_engine_type numerical
\biblio_style plainnat
\biblatex_bibstyle numeric
\biblatex_citestyle numeric
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Reinforcement Learning with Hard Constraints for Autonomous Driving: CS234
 project milestone
\end_layout

\begin_layout Author
Philippe Weingertner, Vaishali G Kulkarni
\begin_inset Newline newline
\end_inset

 
\family typewriter
pweinger@stanford.edu,
\family default
 
\family typewriter
vaishali@stanford.edu
\end_layout

\begin_layout Standard
\align center

\series bold
Project Mentor: Ramtin Keramati
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Reinforcement Learning (RL) has demonstrated its capability to learn efficient
 strategies on many different and complex tasks.
 In particular, in games like chess and go, the best human players have
 lost against RL algorithms (
\begin_inset CommandInset citation
LatexCommand citet
key "silver2017mastering"
literal "false"

\end_inset

).
 There is a growing traction for applying such RL algorithms to complex
 robotics tasks like Autonomous Driving.
 Nevertheless with Autonomous Driving we are dealing with additional challenges.
 We are in a partially observable environment where enforcing safety is
 of paramount importance.
 As a consequence, considering safety via a reward and the optimization
 of a statistical criteria is not sufficient.
 Hard Constraints have to be enforced all the time.
 We propose to study how the RL optimization criteria can be modified to
 deal with hard constraints; how algorithms like DQN could be modified to
 cope with such hard constraints and more generally how an RL agent could
 be integrated in a Decision Making module for Autonomous Driving to provide
 efficient and scalable strategies while still providing safety guarantees.
 So we propose to address the following problem formulation:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\underset{\theta}{max}\:\mathbb{E}[\sum_{t=0}^{\infty}\gamma^{t}R(s_{t},\pi_{\theta}(s_{t}))]
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\text{ s.t. }lower\_bound(C_{i}(s_{t},a_{t}))\geq\text{Margin}_{i}\:\forall i\in\left\llbracket 1,K\right\rrbracket 
\]

\end_inset


\end_layout

\begin_layout Standard
where the expectation corresponds to the statistical RL objective subject
 to a set of safety constraints.
 
\end_layout

\begin_layout Standard
We tackle the problem of safe control in physical systems where certain
 quantities have to be kept constrained.
 For an autonomous vehicle we must always maintain its distance from obstacles
 above some margin.
 But in fact the real state of the world is only partially observable and
 the Driving Models of surrounding cars are not known exactly: so we are
 dealing with uncertainty and our constraints are actually a set of random
 variables 
\begin_inset Formula $C_{i}$
\end_inset

 which we want to lower bound.
 Note that in most of the references the constraints are only considered
 in expectation via a constraint of type 
\begin_inset Formula $J_{C_{i}}^{\pi}=E_{\pi}\left[C_{i}(s,a)\right]$
\end_inset

 whereas here we are interested in enforcing stronger constraints.
\end_layout

\begin_layout Section
Background/Related Work
\end_layout

\begin_layout Standard
In 
\begin_inset CommandInset citation
LatexCommand citet
key "inproceedings"
literal "false"

\end_inset

 a DQN network is used for tactical decision making in an autonomous driving
 pipeline but the DQN algorithm itself is not modified to handle hard constraint
s and the safety is guaranteed by checking the output of the RL algorithm.
 Our objective here, in contrast, would be to have an RL algorithm that
 is directly dealing with hard constraints to avoid frequent and sub-optimal
 actions masking.
 A review of the different safe RL techniques has been done in 
\begin_inset CommandInset citation
LatexCommand citet
key "JMLR:v16:garcia15a"
literal "false"

\end_inset

.
 Some techniques mainly deal with soft constraints by either reshaping the
 reward or trying to minimize the variance related to the risk of making
 unsafe decisions, while other try to handle hard constraints.
 Garcia et al.
 have analyzed and categorized safe RL techniques in two families of approaches:
 one consists in modifying the exploration process while the other consists
 in modifying the optimality criterion.
 In 
\begin_inset CommandInset citation
LatexCommand citet
key "Leurent2018ApproximateRC"
literal "false"

\end_inset

 the RL objective is replaced by a surrogate objective which captures hard
 constraints and handles model uncertainty by defining a lower bound of
 the expectation objective.
 In 
\begin_inset CommandInset citation
LatexCommand citet
key "DBLP:journals/corr/AchiamHTA17"
literal "false"

\end_inset

 constrained policy optimization is solved with a modified trust-region
 policy gradient.
 The algorithm's update rule projects the policy to a safe feasibility set
 in each iteration.
 But the policy is kept within constraints only in expectation.
 In 
\begin_inset CommandInset citation
LatexCommand citet
key "DBLP:journals/corr/abs-1801-08757"
literal "false"

\end_inset

 they directly add to the policy a safety layer that analytically solves
 an action correction formulation per each state.
 This safety layer is learned beforehand but is approximated by a first
 order linear approximator.
 In 
\begin_inset CommandInset citation
LatexCommand citet
key "DBLP:journals/corr/abs-1805-11074"
literal "false"

\end_inset

 and in 
\begin_inset CommandInset citation
LatexCommand citet
key "bohez2019success"
literal "false"

\end_inset

 the proposed approaches are completely in line with our objective here:
 modifying the RL objective such that it deals directly with hard constraints.
 But there is no closed form solution for such a problem and a Lagrangian
 relaxation technique is used for solving the constrained optimization problem.
 Given a Constrained Markov Decision Process (CMDP), the unconstrained problem
 is transformed to 
\begin_inset Formula $\underset{\lambda\geq0}{min}\;\underset{\theta}{max}\;L(\lambda,\theta)=\underset{\lambda\geq0}{min}\;\underset{\theta}{max}\;[J_{R}^{\pi_{\theta}}-\lambda(J_{C}^{\pi_{\theta}}-\alpha)]$
\end_inset

 where 
\begin_inset Formula $L$
\end_inset

 is the Lagrangian and 
\begin_inset Formula $\lambda$
\end_inset

 the Lagrange multiplier (a penalty coefficient).
 We propose to study how such techniques could be applied to the Decision
 Making process of an Autonomous Driving pipeline and we will benchmark
 different RL algorithms, modified to cope with hard constraints, in an
 Anti Collision Tests setting.
\end_layout

\begin_layout Section
Approach
\end_layout

\begin_layout Standard
A DQN network will be used as a baseline (we may change later to Policy
 Gradients or Actor Critic.
 This is a topic for further refinement).
 We will consider 3 type of modifications.
 From the most simple, conceptually, to the most complex, we will:
\end_layout

\begin_layout Enumerate
Propose a post DQN safety check.
 While the DQN network will compute 
\begin_inset Formula $Q(s,a_{i})$
\end_inset

 values for every possible actions, we want to exclude the actions that
 are unsafe, before deciding what action to take (before taking the 
\begin_inset Formula $argmax_{a_{i}}\;Q(s,a_{i})$
\end_inset

).
 This type of approach is used in a paper from BMW Group by 
\begin_inset CommandInset citation
LatexCommand citet
key "inproceedings"
literal "true"

\end_inset

.
\end_layout

\begin_layout Enumerate
Modify the DQN training algorithm and especially the exploration process
 so that only safe actions are explored.
 Similar to 
\begin_inset CommandInset citation
LatexCommand citet
key "Bouton2018uai"
literal "true"

\end_inset

, the idea is to derive an exploration strategy that constrains the agent
 to choose among actions that satisfy safety criteria.
 Hence the search space of policies is restricted to a 
\begin_inset Quotes eld
\end_inset

safe
\begin_inset Quotes erd
\end_inset

 or safer subspace of policies.
\end_layout

\begin_layout Enumerate
Replace the RL objective 
\begin_inset Formula $\underset{\theta}{max}\:\mathbb{E}[\sum_{t=0}^{\infty}\gamma^{t}R(s_{t},\pi_{\theta}(s_{t}))]$
\end_inset

 by an objective taking into account hard constraints 
\begin_inset Formula 
\[
\underset{\theta}{max}\:\mathbb{E}[\sum_{t=0}^{\infty}\gamma^{t}R(s_{t},\pi_{\theta}(s_{t}))]
\]

\end_inset


\begin_inset Formula 
\[
\text{ s.t. }lower\_bound(C_{i}(s_{t},a_{t}))\geq\text{Margin}_{i}\:\forall i\in\left\llbracket 1,K\right\rrbracket 
\]

\end_inset

 and study how RL algorithms like DQN should be modified to account for
 this new objective.
 In a recent paper from DeepMind from 
\begin_inset CommandInset citation
LatexCommand citet
key "bohez2019success"
literal "true"

\end_inset

, this type of approach is applied to a realistic, energy-optimized robotic
 locomotion task, using the Minitaur quadruped developed by 
\begin_inset CommandInset href
LatexCommand href
name " Ghost Robotics"
target "https://www.ghostrobotics.io/"
literal "false"

\end_inset

.
 But the constraints were considered only in expectation.
\end_layout

\begin_layout Section
Experimental Results
\end_layout

\begin_layout Subsection
Simulator
\end_layout

\begin_layout Standard
We will upgrade the Anti Collision Tests environment, 
\begin_inset CommandInset href
LatexCommand href
name "ACT"
target "https://github.com/PhilippeW83440/ACT"
literal "false"

\end_inset

, developed for a previous CS238 Stanford project.
 A vehicle has to drive from a point A to a point B as fast as possible,
 while avoiding other vehicles that are crossing its path and trying to
 minimize hard braking decisions.
 So it is a multi-objectives task where efficiency, comfort and safety objective
s have to be optimized.
 While in the previous project we studied the applicability of POMDPs solvers
 for decision making in a context of sensors uncertainty, we will deal here
 with an even more challenging task: the uncertainty will be related to
 the other vehicles driving models.
 Initially other vehicles driving models were simple Constant Velocity models.
 Here we will use Intelligent Driver Models, 
\begin_inset CommandInset href
LatexCommand href
name "IDM"
target "https://en.wikipedia.org/wiki/Intelligent_driver_model"
literal "false"

\end_inset

, depending on 5 parameters that will be unknown to the ego vehicle, and
 that will differ per vehicle.
 So it is a model-free setting: we do not know the model of the environment,
 the driving models of others, and we would like to learn to drive efficiently
 and safely in this context.
\end_layout

\begin_layout Subsection
Evaluation Metrics
\end_layout

\begin_layout Standard
In order to measure success and benchmark different versions of the algorithms,
 we will use 3 metrics: a safety metric (percentage of collisions), an efficienc
y metric (time to goal), and a comfort metric (number of hard braking decisions
 or jerk).
 We want to enforce safety while not compromising too much efficiency or
 comfort: a safe AD vehicle that would use many regular hard braking decisions
 would not be acceptable and could even be dangerous for other vehicles.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintAll"
bibfiles "safeRL"

\end_inset


\end_layout

\end_body
\end_document
