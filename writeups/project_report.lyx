#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage[final]{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2016}

%\usepackage[utf8]{inputenc} % allow utf-8 input
%\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{smartdiagram}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package babel
\inputencoding utf8
\fontencoding T1
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype true
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement H
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine biblatex
\cite_engine_type numerical
\biblio_style plainnat
\biblatex_bibstyle numeric
\biblatex_citestyle numeric
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Reinforcement Learning with Hard Constraints for Autonomous Driving
\end_layout

\begin_layout Author
Philippe Weingertner, Vaishali G Kulkarni
\begin_inset Newline newline
\end_inset

 
\family typewriter
pweinger@stanford.edu,
\family default
 
\family typewriter
vaishali@stanford.edu
\end_layout

\begin_layout Standard
\align center

\series bold
Project Mentor: Ramtin Keramati
\end_layout

\begin_layout Section
Questions
\end_layout

\begin_layout Subsection
What is the problem?
\end_layout

\begin_layout Standard

\series bold
Why is it important? What are the key limitations of prior work? What are
 you proposing to do? What will that allow us to do now? (e.g.
 it addresses some prior key limitations, is computationally faster, etc
 etc)
\end_layout

\begin_layout Standard
We consider the problem of decision making for an autonomous car.
 The Autonoumous Driving pipeline consists of 3 parts:
\end_layout

\begin_layout Enumerate
Perception: sensors, localization and sensors fusion provide a world model
 that will be used for scene understanding and decision making.
\end_layout

\begin_layout Enumerate
Planning: based on the above world model, decisions like accelerate, slow
 down or change lane are taken and a trajectory is planned.
\end_layout

\begin_layout Enumerate
Control: the planned trajectory is followed as close as possible taking
 into account a detailed dynamical vehicle model and actuators command are
 sent to control the vehicle.
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename img/pipeline.png
	lyxscale 30
	scale 30

\end_inset


\end_layout

\begin_layout Standard
We focus on the Planning module.
 This module is typicaly further decomposed into 3 sub-modules:
\end_layout

\begin_layout Enumerate
A prediction module: predicting the trajectories and intents of other drivers.
 Typically a Driving Model can be inferred in real time to match the behavior
 of other drivers and to anticipate what they could do in the future.
\end_layout

\begin_layout Enumerate
A decision making module: the decisions are typically abstract and higher
 level like change lane but can also be low level like change longitudinal
 or lateral accelerations.
 
\end_layout

\begin_layout Enumerate
A motion planning module: based an above higher level decision, a trajectory
 will be planned
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename img/urban_lidar.png
	lyxscale 50
	scale 50

\end_inset


\end_layout

\begin_layout Standard
The difficulty in Decision Making for Autonomous Driving is due to the combinati
on of different challenges but the main challenge we would like to adress
 here is how to make good decisions in a situation for which we have:
\end_layout

\begin_layout Itemize
No established driving model for other vehicles.
 Every driver is different and may behave differently.
 So by construction we are in a model free setting.
 We do not know the driving model of other drivers.
 We can try to predict such a model, to estimate it, but in reality there
 is no absolute ground truth.
 And the way someone is driving may change suddenly.
 This is the main difficulty: we are faced with a 
\series bold
model free 
\series default
setting, with 
\series bold
uncertainties
\series default
 and 
\series bold
non stationnary behaviors
\series default
.
\end_layout

\begin_layout Itemize

\series bold
Safety requirements
\series default
: we have to make safe decisions to avoid collisions and not just 99% of
 the time.
 Nevertheless there are 2 type of collisions, responsible and non-responsible
 ones.
 There will never be a guarantee for 0% collisions, but we have to avoid
 
\begin_inset Quotes eld
\end_inset

responsible collisions
\begin_inset Quotes erd
\end_inset

 and in the remaining cases, for non-responsible collisions, we have to
 make decisions that diminish the consequences of a collision.
\end_layout

\begin_layout Standard
This problem is of paramount importance and very challenging.
 The model free setting, where we do not know the driving model of others,
 is a good match for a RL model free formulation of the problem.
 But the safety requirements are handled with rules we want to enforce and
 check.
 With a RL solution we are defining an objective that is optimized in expectatio
n.
 It can be very good in expectation.
 But if it has variance, we may occasionaly fail the safety requirements.
 So in terms of problem formulation we would like to add hard constraints
 to an objective in expectation.
\end_layout

\begin_layout Standard
Now let's consider further what these Safety Constraints could be.
 Ultimately we want to avoid collisions.
 But this is a very late signal and we want to use a signal, that we can
 constrain and use ahead of time to prevent collisions or to diminish the
 consequences of collisions (in case they can not be avoided).
 A signal of interest is the Time To Collision: it is a sort of a proxy
 signal for the collisions we can explicit.
 Once available we can define constraints on it.
 So typically the constraint we will use is that the minimum Time To Collision
 shall be above some margin.
 As we are dealing with uncertainties, we do not know the driving models
 of other, the TTC or minTTC is actually a Random Variable.
 So ultimately in the way we compute the TTC or minTTC and the way we set
 constraints, we have to account for uncertainties.
 As a quick summary:
\end_layout

\begin_layout Enumerate
An AD pipeline typically predicts the Driver Model or trajectories of others.
 Let's assume it tries to best match an IDM/MOBIL driver model for every
 surrounding car.
 The IDM driver model enables to predict longitudinal acceleration, it depends
 on 5 parameters, whereas the MOBIL driver model enables to predict the
 lateral acceleration and depends on 3 parameters (one parameter being a
 level of politeness).
 These models are much more valuable than raw trajectories prediction, because
 as input they take into account contextual information: like relative speeds
 or distances.
 So they take into account what other cars do, to predict what the car of
 interest will do.
 And if the context change, the prediction will change.
 Whereas simple models like CV (Constant Velocity) or CA (Constant Acceleration)
 do not take into account contextual information.
 So typically CV or CA models may be reliable for very short time horizons
 whereas IDM/MOBIL models are very usefull for longer time horizons or when
 a driver is doing something more complex like a lane change or lane merge.
\end_layout

\begin_layout Enumerate
Once we have driving models available for surronding cars, we can evaluate
 the minTTC.
 To simplify things, assume we are in a scene with 2 cars driving longitudinally.
 The prediction module has best fitted 2 IDM models with 5 parameters each.
 And estimated the range of uncertainties for every parameter.
 Based on this we can define a conservative or robust TTC value: we sample
 possible trajectories and account for the min TTC obtained.
\end_layout

\begin_layout Standard
And ultimately we want to have a constraint on this minTTC: we would like
 to enforce that 
\begin_inset Formula $minTTC\geq margin$
\end_inset

 
\end_layout

\begin_layout Standard
Now this is where the main challenge is when considering RL with Hard Constraint
s.
 The constraint that is of interest to us here, is very complex to compute.
 There is no simple differentiable graph that could be defined to express
 this constraint.
 With above IDM models: we would iterate over time steps for every cars,
 and once we know the new position of every other car we can estimate the
 longitudinal acceleration of one car and so on ...
 And on top of that to account for uncertainties we sample IDM parameters.
 So the hard constraint computation is a non trivial piece of code that
 is not well suited for expression as a Tensorflow Graph.
 Most of the papers dealing with RL and hard constraints assume the constraint
 is expressed as a mathematical expression that is differentiable.
 So all the methods dealing with Lagrangian formulation or Lyapunov stability
 analysis are not applicable to our case.
 There tend to be lots of mathematical derivations in most of the papers,
 but ultimately these methods are applicable in restricted cases and usualy
 tested on relativelly toy constraints (like a torque constraint).
 In some cases, when the constraint is not differentiable, a differentiable
 model is learned to best match the original constraint.
 The problem we are considering here, in its most generic formulation, is
 how to best handle complex, non differentiable constraints, in an RL setting.
 Anoter key point to consider, is that we have to handle unsafe states,
 being in an unsafe state (collision risk, minTTC<=10), identifying it as
 unsafe and trying to move as fast as we can to safer states.
 In some paper like CPO/TRPO they consider they start in a safe region and
 try to remain in safe regions while learning new parameters for the Policy
 Network.
 So our setting and objectives are different as well: we evaluate a safety
 cost, if it does not match our hard constraint, we try to improve to match
 it and then keep it above the hard constraint.
 So we try to deal with a recovery phase if the hard constraint is not matched.
 And the main difficulty of the setting is that we are dealing with moving
 objects for which we have no ground truth models.
\end_layout

\begin_layout Standard
We developped an openai gym, module that enable to experiment with some
 of the challenges.
 
\end_layout

\begin_layout Subsection
What did you do?
\end_layout

\begin_layout Subsubsection
Implementation of an openai gym module
\end_layout

\begin_layout Standard
We developped an openai gym test setup for experimentations.
 We used to have some early code in Julia used in a previous Stanford CS238
 course but we migrated everything to Python and properly developped a generic
 easy to instal and run open ai gym module.
 It is a test scene where a car, which we call the ego-car, has to drive
 from point A to point B as fast as possible while othe cars are crossing
 its path.
 Other cars may be instantiated randomly at different positions, with different
 speeds.
 They may use different driver models: CV, Basic, IDM.
 It is a model free setting.
 We are trying to learn an agent that can drive efficiently (as fast as
 possible from point A to point B with realistic bounded accelerations 
\begin_inset Formula $a\in[-2;2]ms^{-2}$
\end_inset

) and safely (while minimizing the percentage of collisions).
 
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename img/act_env.png
	lyxscale 30
	scale 30

\end_inset


\end_layout

\begin_layout Standard
The problem is setup as a MDP with:
\end_layout

\begin_layout Itemize
States: different variants will be tested, but as a starting point consider
 
\begin_inset Formula $\ensuremath{\{(x,y,v_{x},v_{y})_{ego},{(x,y,v_{x},v_{y})_{obj}}_{1..n}\}}$
\end_inset


\end_layout

\begin_layout Itemize
Actions: longitudinal accelerations 
\begin_inset Formula $a_{longi}\in\left[-2ms^{-2};+2ms^{-2}\right]$
\end_inset


\end_layout

\begin_layout Itemize
Rewards: 
\begin_inset Formula $-1$
\end_inset

for every timestep,
\begin_inset Formula $-1000$
\end_inset

 for a collision terminal state, 
\begin_inset Formula $+1000$
\end_inset

 when goal is reahced terminal state
\end_layout

\begin_layout Itemize
Model-Free setting: the ego-vehicle does not know the driver model of other
 cars, it could be CV, Basic or IDM.
\end_layout

\begin_layout Standard
An openai gym package has beeen developed and can be easilly used with any
 existing RL framework.
 It supports all the standard openai gym API step, reset, render etc and
 custom ones have been added to experiment with safety constraints and penalties.
\end_layout

\begin_layout Subsubsection
Policy Gradients based algorithms benchmark: VPG, PPO, TRPO, DDPG, TD3,
 SAC
\end_layout

\begin_layout Standard
Default setting.
 Test on Act-v0.
 SAC performs best.
 
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename img/benchmark.png
	lyxscale 40
	scale 40

\end_inset


\end_layout

\begin_layout Subsubsection
Experiments on different state space representations and how it scale
\end_layout

\begin_layout Itemize
With vectorized absolute state space representation
\end_layout

\begin_layout Itemize
With vectorized relative state space representation
\end_layout

\begin_layout Itemize
With vectorized reduced state space representation
\end_layout

\begin_layout Itemize
With image state space representation
\end_layout

\begin_layout Subsubsection
Implementation and Experiments on how to handle complex non differentiable
 safety constraints and penalties
\end_layout

\begin_layout Itemize
Reward shaping with a penalty term
\end_layout

\begin_layout Itemize
Line search based on penalty constraint
\end_layout

\begin_layout Standard
Backtracking Line Search over a batch of 50000 states tested over different
 policy networks 
\begin_inset Formula $\pi_{\theta}$
\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $\theta\gets\theta+\alpha_{bls}\nabla\pi_{\theta}$
\end_inset

 is tested for different step sizes with 
\begin_inset Formula $\alpha_{bls}<\alpha_{sgd}$
\end_inset

 
\end_layout

\begin_layout Standard
So we have a 2-stages Gradient Descent, with 2 interleaved steps:
\end_layout

\begin_layout Itemize
The first step deals with a differentiable function trying to optimize 
\begin_inset Formula $Q(s,a)$
\end_inset

 in expectation
\end_layout

\begin_layout Itemize
While the 2nd stage is refining the step size 
\begin_inset Formula $\alpha$
\end_inset

 over a non differentiable safety penalty to search for safer policy networks.
\end_layout

\begin_layout Standard
By definition a lot of states are unsafe.
 They correspond to a predicted 
\begin_inset Formula $minTTC<10$
\end_inset

 seconds.
 we are trying to come up with a policy that pushes us towards safer states
 with bounded accelerations at every time step or that ensures we remain
 in safe states if this is already the case.
 But we can not move in a single step from an unsafe state to a safer state.
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

Time to compute penalty: 17.25
\end_layout

\begin_layout Plain Layout

old_penalty 326022.49
\end_layout

\begin_layout Plain Layout

sgd_penalty 322133.54
\end_layout

\begin_layout Plain Layout

Average reward: -1881.22 +/- 10.82
\end_layout

\begin_layout Plain Layout

Time to compute penalty: 17.11
\end_layout

\begin_layout Plain Layout

old_penalty 316778.55
\end_layout

\begin_layout Plain Layout

sgd_penalty 354335.09
\end_layout

\begin_layout Plain Layout

Backtracking bt_penalty 348790.05
\end_layout

\begin_layout Plain Layout

Backtracking: improvement at iter 0 bt_penalty=348790.05 sgd_penalty=354335.09
\end_layout

\begin_layout Plain Layout

Backtracking Time: 17.31
\end_layout

\begin_layout Plain Layout

Average reward: -1773.23 +/- 9.44
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
What experiments did you run, what did you find out.
 It's okay if your new idea wasn't better.
 But if so, it's important to have ideas about why it didn't work.
\end_layout

\begin_layout Subsection
What are next steps / open questions that if you were to continue working,
 you would do?
\end_layout

\begin_layout Itemize
Curse of dimensinality: more efficient state space representation
\end_layout

\begin_layout Itemize
Line search is slow: learn where to search first ? 
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Reinforcement Learning (RL) has demonstrated its capability to learn efficient
 strategies on many different and complex tasks.
 In particular, in games like chess and go, the best human players have
 lost against RL algorithms (
\begin_inset CommandInset citation
LatexCommand citet
key "silver2017mastering"
literal "false"

\end_inset

).
 There is a growing traction for applying such RL algorithms to complex
 robotics tasks like Autonomous Driving.
 Nevertheless with Autonomous Driving we are dealing with additional challenges.
 We are in a partially observable environment where enforcing safety is
 of paramount importance.
 As a consequence, considering safety via a reward and the optimization
 of a statistical criteria is not sufficient.
 Hard Constraints have to be enforced all the time.
 We propose to study how the RL optimization criteria can be modified to
 deal with hard constraints; how algorithms like DQN could be modified to
 cope with such hard constraints and more generally how an RL agent could
 be integrated in a Decision Making module for Autonomous Driving to provide
 efficient and scalable strategies while still providing safety guarantees.
 So we propose to address the following problem formulation:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\underset{\theta}{max}\:\mathbb{E}[\sum_{t=0}^{\infty}\gamma^{t}R(s_{t},\pi_{\theta}(s_{t}))]
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\text{ s.t. }lower\_bound(C_{i}(s_{t},a_{t}))\geq\text{Margin}_{i}\:\forall i\in\left\llbracket 1,K\right\rrbracket 
\]

\end_inset


\end_layout

\begin_layout Standard
where the expectation corresponds to the statistical RL objective subject
 to a set of safety constraints.
 
\end_layout

\begin_layout Standard
We tackle the problem of safe control in physical systems where certain
 quantities have to be kept constrained.
 For an autonomous vehicle we must always maintain its distance from obstacles
 above some margin.
 But in fact the real state of the world is only partially observable and
 the Driving Models of surrounding cars are not known exactly: so we are
 dealing with uncertainty and our constraints are actually a set of random
 variables 
\begin_inset Formula $C_{i}$
\end_inset

 which we want to lower bound.
 Note that in most of the references the constraints are only considered
 in expectation via a constraint of type 
\begin_inset Formula $J_{C_{i}}^{\pi}=E_{\pi}\left[C_{i}(s,a)\right]$
\end_inset

 whereas here we are interested in enforcing stronger constraints.
\end_layout

\begin_layout Standard

\series bold
Project Milestone Updates:
\end_layout

\begin_layout Itemize
We will use a single hard constraint which is related to the min Time To
 Collision that is computed between the ego vehicle and the predicted trajectori
es of the surrounding vehicles.
 This min TTC shall be greater or equal than a threshold margin.
\end_layout

\begin_layout Section
Background/Related Work
\end_layout

\begin_layout Standard
In 
\begin_inset CommandInset citation
LatexCommand citet
key "inproceedings"
literal "false"

\end_inset

 a DQN network is used for tactical decision making in an autonomous driving
 pipeline but the DQN algorithm itself is not modified to handle hard constraint
s and the safety is guaranteed by checking the output of the RL algorithm.
 Our objective here, in contrast, would be to have an RL algorithm that
 is directly dealing with hard constraints to avoid frequent and sub-optimal
 actions masking.
 A review of the different safe RL techniques has been done in 
\begin_inset CommandInset citation
LatexCommand citet
key "JMLR:v16:garcia15a"
literal "false"

\end_inset

.
 Some techniques mainly deal with soft constraints by either reshaping the
 reward or trying to minimize the variance related to the risk of making
 unsafe decisions, while other try to handle hard constraints.
 Garcia et al.
 have analyzed and categorized safe RL techniques in two families of approaches:
 one consists in modifying the exploration process while the other consists
 in modifying the optimality criterion.
 In 
\begin_inset CommandInset citation
LatexCommand citet
key "Leurent2018ApproximateRC"
literal "false"

\end_inset

 the RL objective is replaced by a surrogate objective which captures hard
 constraints and handles model uncertainty by defining a lower bound of
 the expectation objective.
 In 
\begin_inset CommandInset citation
LatexCommand citet
key "DBLP:journals/corr/AchiamHTA17"
literal "false"

\end_inset

 constrained policy optimization is solved with a modified trust-region
 policy gradient.
 The algorithm's update rule projects the policy to a safe feasibility set
 in each iteration.
 But the policy is kept within constraints only in expectation.
 In 
\begin_inset CommandInset citation
LatexCommand citet
key "DBLP:journals/corr/abs-1801-08757"
literal "false"

\end_inset

 they directly add to the policy a safety layer that analytically solves
 an action correction formulation per each state.
 This safety layer is learned beforehand but is approximated by a first
 order linear approximator.
 In 
\begin_inset CommandInset citation
LatexCommand citet
key "DBLP:journals/corr/abs-1805-11074"
literal "false"

\end_inset

 and in 
\begin_inset CommandInset citation
LatexCommand citet
key "bohez2019success"
literal "false"

\end_inset

 the proposed approaches are completely in line with our objective here:
 modifying the RL objective such that it deals directly with hard constraints.
 But there is no closed form solution for such a problem and a Lagrangian
 relaxation technique is used for solving the constrained optimization problem.
 Given a Constrained Markov Decision Process (CMDP), the unconstrained problem
 is transformed to 
\begin_inset Formula $\underset{\lambda\geq0}{min}\;\underset{\theta}{max}\;L(\lambda,\theta)=\underset{\lambda\geq0}{min}\;\underset{\theta}{max}\;[J_{R}^{\pi_{\theta}}-\lambda(J_{C}^{\pi_{\theta}}-\alpha)]$
\end_inset

 where 
\begin_inset Formula $L$
\end_inset

 is the Lagrangian and 
\begin_inset Formula $\lambda$
\end_inset

 the Lagrange multiplier (a penalty coefficient).
 We propose to study how such techniques could be applied to the Decision
 Making process of an Autonomous Driving pipeline and we will benchmark
 different RL algorithms, modified to cope with hard constraints, in an
 Anti Collision Tests setting.
\end_layout

\begin_layout Section
Approach
\end_layout

\begin_layout Standard
A DQN network will be used as a baseline (we may change later to Policy
 Gradients or Actor Critic.
 This is a topic for further refinement).
 We will consider 3 type of modifications.
 From the most simple, conceptually, to the most complex, we will:
\end_layout

\begin_layout Enumerate
Propose a post DQN safety check.
 While the DQN network will compute 
\begin_inset Formula $Q(s,a_{i})$
\end_inset

 values for every possible actions, we want to exclude the actions that
 are unsafe, before deciding what action to take (before taking the 
\begin_inset Formula $argmax_{a_{i}}\;Q(s,a_{i})$
\end_inset

).
 This type of approach is used in a paper from BMW Group by 
\begin_inset CommandInset citation
LatexCommand citet
key "inproceedings"
literal "true"

\end_inset

.
\end_layout

\begin_layout Enumerate
Modify the DQN training algorithm and especially the exploration process
 so that only safe actions are explored.
 Similar to 
\begin_inset CommandInset citation
LatexCommand citet
key "Bouton2018uai"
literal "true"

\end_inset

, the idea is to derive an exploration strategy that constrains the agent
 to choose among actions that satisfy safety criteria.
 Hence the search space of policies is restricted to a 
\begin_inset Quotes eld
\end_inset

safe
\begin_inset Quotes erd
\end_inset

 or safer subspace of policies.
\end_layout

\begin_layout Enumerate
Replace the RL objective 
\begin_inset Formula $\underset{\theta}{max}\:\mathbb{E}[\sum_{t=0}^{\infty}\gamma^{t}R(s_{t},\pi_{\theta}(s_{t}))]$
\end_inset

 by an objective taking into account hard constraints 
\begin_inset Formula 
\[
\underset{\theta}{max}\:\mathbb{E}[\sum_{t=0}^{\infty}\gamma^{t}R(s_{t},\pi_{\theta}(s_{t}))]
\]

\end_inset


\begin_inset Formula 
\[
\text{ s.t. }lower\_bound(C_{i}(s_{t},a_{t}))\geq\text{Margin}_{i}\:\forall i\in\left\llbracket 1,K\right\rrbracket 
\]

\end_inset

 and study how RL algorithms like DQN should be modified to account for
 this new objective.
 In a recent paper from DeepMind from 
\begin_inset CommandInset citation
LatexCommand citet
key "bohez2019success"
literal "true"

\end_inset

, this type of approach is applied to a realistic, energy-optimized robotic
 locomotion task, using the Minitaur quadruped developed by 
\begin_inset CommandInset href
LatexCommand href
name " Ghost Robotics"
target "https://www.ghostrobotics.io/"
literal "false"

\end_inset

.
 But the constraints were considered only in expectation.
\end_layout

\begin_layout Standard

\series bold
Project Milestone Updates:
\end_layout

\begin_layout Itemize
We will focus on step 3 
\end_layout

\begin_layout Itemize
The surrounding vehicles will move according to an 
\begin_inset CommandInset href
LatexCommand href
name "IDM "
target "https://en.wikipedia.org/wiki/Intelligent_driver_model"
literal "false"

\end_inset

 driving model.
 This driving model depends on 5 parameters.
 The ego vehicle does not know (precisely) the parameters used by the surroundin
g vehicles.
 This is the main source of uncertainty.
\end_layout

\begin_layout Itemize
In order to define the hard constraint, in a real use case (not simulated)
 the ego vehicle shall estimate the Driving Model parameters of every surroundin
g vehicle with some probability distribution to account for uncertainty.
 Here in simulation and in the context of this project, we will use some
 provided range of IDM parameters values: so the IDM model estimation is
 emulated.
 Then we will consider worst case scenarios leading to the smallest possible
 Time To Collision and assess how robust we are againt different level of
 uncertainties.
\end_layout

\begin_layout Itemize
So the hard constraint will be 
\begin_inset Formula $\text{min TTC }\geq10$
\end_inset

 where the Time To Collision is computed based on:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $acceleration_{ego-vehicle}=\pi_{\theta}(s_{t})$
\end_inset

 which depends on the parameters of the Neural Network
\end_layout

\begin_layout Itemize
Predicted trajectories of surrounding vehicles which depend on a set of
 5 IDM parameters per surrounding vehicle.
 We will simulate the fact that these parameters are estimated by the ego
 vehicle by providing range of values to the ego vehicle in the simulation
 framework.
 
\end_layout

\begin_layout Itemize
So the hard constraint will involve the parameters 
\begin_inset Formula $\theta$
\end_inset

 of 
\begin_inset Formula $\pi_{\theta}(s_{t})$
\end_inset

 such that min Time To Collision is above some threshold.
\end_layout

\end_deeper
\begin_layout Itemize
In terms of implementing the hard constraint with Tensorflow we will most
 probably leverage on the following code 
\begin_inset CommandInset href
LatexCommand href
name "tensorflow_constrained_optimization"
target "https://github.com/google-research/https://github.com/google-research/tensorflow_constrained_optimization"
literal "false"

\end_inset

 from Google research.
\end_layout

\begin_layout Section
Experimental Results
\end_layout

\begin_layout Standard

\series bold
Project Milestone Updates:
\end_layout

\begin_layout Itemize
A github directory has been created for the project: 
\begin_inset CommandInset href
LatexCommand href
name "CS234_Project"
target "https://github.com/PhilippeW83440/CS234_Project"
literal "false"

\end_inset


\end_layout

\begin_layout Itemize
The legacy simulator code has been ported from Julia code (as used in CS238
 project) to Python (for use in CS234 Project)
\end_layout

\begin_layout Itemize
The simulator code has been upgraded so that the surrounding vehicles move
 according to an 
\begin_inset CommandInset href
LatexCommand href
name "IDM "
target "https://en.wikipedia.org/wiki/Intelligent_driver_model"
literal "false"

\end_inset

 driving model.
 In previous CS238 project the surrounding vehicles were moving according
 to a very simple Constant Velocity model.
\end_layout

\begin_layout Subsection
Simulator
\end_layout

\begin_layout Standard
We are upgrading the Anti Collision Tests environment, 
\begin_inset CommandInset href
LatexCommand href
name "ACT"
target "https://github.com/PhilippeW83440/ACT"
literal "false"

\end_inset

, developed for a previous CS238 Stanford project.
 A vehicle has to drive from a point A to a point B as fast as possible,
 while avoiding other vehicles that are crossing its path and trying to
 minimize hard braking decisions.
 So it is a multi-objectives task where efficiency, comfort and safety objective
s have to be optimized.
 While in the previous project we studied the applicability of POMDPs solvers
 for decision making in a context of sensors uncertainty, we will deal here
 with an even more challenging task: the uncertainty will be related to
 the other vehicles driving models.
 Initially other vehicles driving models were simple Constant Velocity models.
 Here we will use Intelligent Driver Models, 
\begin_inset CommandInset href
LatexCommand href
name "IDM"
target "https://en.wikipedia.org/wiki/Intelligent_driver_model"
literal "false"

\end_inset

, depending on 5 parameters that will be unknown to the ego vehicle, and
 that will differ per vehicle.
 So it is a model-free setting: we do not know the model of the environment,
 the driving models of others, and we would like to learn to drive efficiently
 and safely in this context.
\end_layout

\begin_layout Subsection
Evaluation Metrics
\end_layout

\begin_layout Standard
In order to measure success and benchmark different versions of the algorithms,
 we will use 3 metrics: a safety metric (percentage of collisions), an efficienc
y metric (time to goal), and a comfort metric (number of hard braking decisions
 or jerk).
 We want to enforce safety while not compromising too much efficiency or
 comfort: a safe AD vehicle that would use many regular hard braking decisions
 would not be acceptable and could even be dangerous for other vehicles.
\end_layout

\begin_layout Section
Project Milestones Updates: Remaining Work / Next Steps
\end_layout

\begin_layout Itemize
Establish a baseline with RL Policy Gradient algorithm trained on ACT simulator
 framework.
 Provide evaluation metrics results.
\end_layout

\begin_layout Itemize
Meeting to be planned 
\begin_inset Formula $5^{th}$
\end_inset

 or 
\begin_inset Formula $6^{th}$
\end_inset

 of March with Vaishali, Ramtin, Philippe.
 Philippe who is located in France will be in California these days.
\end_layout

\begin_layout Itemize
Experiment with 
\begin_inset CommandInset href
LatexCommand href
name "tensorflow_constrained_optimization"
target "https://github.com/google-research/https://github.com/google-research/tensorflow_constrained_optimization"
literal "false"

\end_inset

 from Google research on a simple hard constraint use case (e.g.
 such that the weights of the Neural Network are in a specific range)
\end_layout

\begin_layout Itemize
Implement code to compute the hard constraint 
\begin_inset Formula $\text{ min TTC }\geq$
\end_inset

 some threshold.
 In a first step just log how we would deviate from our goal by not enforcing
 the hard constraint
\end_layout

\begin_layout Itemize
Combine Policy Gradient algorithm with the Hard Constraint code during Policy
 Gradient training
\end_layout

\begin_layout Itemize
Compare results with and without enforcing the hard constraint
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintAll"
bibfiles "safeRL"

\end_inset


\end_layout

\end_body
\end_document
