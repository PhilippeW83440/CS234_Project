{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-03-14 12:32:21,763] Making new env: Act-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACT (Anti Collision Tests) with 2 cars using cv driver model\n",
      "SEED 329655221806033094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philippew/anaconda3/envs/py36/lib/python3.6/site-packages/gym/envs/registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "# to install gym_act:\n",
    "# git clone https://github.com/PhilippeW83440/CS234_Project.git\n",
    "# cd gym-act\n",
    "# pip install -e .\n",
    "import gym_act\n",
    "\n",
    "# source code is in https://github.com/PhilippeW83440/CS234_Project/blob/master/gym-act/gym_act/envs/act_env.py\n",
    "# or if you did: git clone https://github.com/PhilippeW83440/CS234_Project.git\n",
    "# in CS234_Project/gym-act/gym_act/envs/act_env.py\n",
    "\n",
    "# By default: ACT with 2 cars with CV (Constant Velocity) driver model\n",
    "env = gym.make(\"Act-v0\")\n",
    "\n",
    "# ACT with 10 cars with CV (Constant Velocity) driver model\n",
    "#env = gym.make(\"Act10cv-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward -1\n"
     ]
    }
   ],
   "source": [
    "action = 0\n",
    "obs, reward, done, info = env.step(action)\n",
    "print(\"reward {}\".format(reward))\n",
    "img = env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "def show_img(img):\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "def  resize_images(images, f=3):\n",
    "    big_images = []\n",
    "    for img in images:\n",
    "        big_images.append(cv2.resize(img, None, fx=f, fy=f))\n",
    "    return big_images\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"]=10,10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a trained Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import tensorflow as tf\n",
    "from spinup.utils.logx import restore_tf_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json    simple_save12  simple_save5  vars11.pkl\tvars3.pkl  vars9.pkl\r\n",
      "progress.txt   simple_save13  simple_save6  vars12.pkl\tvars4.pkl  vars.pkl\r\n",
      "simple_save    simple_save14  simple_save7  vars13.pkl\tvars5.pkl\r\n",
      "simple_save1   simple_save2   simple_save8  vars14.pkl\tvars6.pkl\r\n",
      "simple_save10  simple_save3   simple_save9  vars1.pkl\tvars7.pkl\r\n",
      "simple_save11  simple_save4   vars10.pkl    vars2.pkl\tvars8.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/td3/td3_s0/\n",
    "fpath='data/td3/td3_s0/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "itr='last'\n",
    "# handle which epoch to load from\n",
    "if itr=='last':\n",
    "    saves = [int(x[11:]) for x in os.listdir(fpath) if 'simple_save' in x and len(x)>11]\n",
    "    itr = '%d'%max(saves) if len(saves) > 0 else ''\n",
    "else:\n",
    "    itr = '%d'%itr\n",
    "print(itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = restore_tf_graph(sess, osp.join(fpath, 'simple_save'+itr))\n",
    "action_op = model['pi']\n",
    "\n",
    "# make function for producing an action given a single state\n",
    "get_action = lambda x : sess.run(action_op, feed_dict={model['x']: x[None,:]})[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load env used for training the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "state = joblib.load(osp.join(fpath, 'vars'+itr+'.pkl'))\n",
    "env = state['env']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect metrics on trained Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Start episode 0\n",
      "Step 0: action=[2.] reward=-1 done=False\n",
      "Step 1: action=[2.] reward=-1 done=False\n",
      "Step 2: action=[2.] reward=-1 done=False\n",
      "Step 3: action=[2.] reward=-1 done=False\n",
      "Step 4: action=[2.] reward=-1 done=False\n",
      "Step 5: action=[2.] reward=-1 done=False\n",
      "Step 6: action=[2.] reward=-1 done=False\n",
      "Step 7: action=[2.] reward=-1 done=False\n",
      "Step 8: action=[2.] reward=-1 done=False\n",
      "Step 9: action=[2.] reward=-1 done=False\n",
      "Step 10: action=[2.] reward=-1 done=False\n",
      "Step 11: action=[2.] reward=-1 done=False\n",
      "Step 12: action=[2.] reward=-1 done=False\n",
      "Step 13: action=[2.] reward=-1 done=False\n",
      "Step 14: action=[2.] reward=-1 done=False\n",
      "Step 15: action=[2.] reward=-1 done=False\n",
      "Step 16: action=[2.] reward=-1 done=False\n",
      "Step 17: action=[2.] reward=-1 done=False\n",
      "Step 18: action=[2.] reward=-1 done=False\n",
      "Step 19: action=[2.] reward=-1 done=False\n",
      "Step 20: action=[2.] reward=-1 done=False\n",
      "Step 21: action=[2.] reward=-1 done=False\n",
      "Step 22: action=[2.] reward=-1 done=False\n",
      "Step 23: action=[2.] reward=-1 done=False\n",
      "Step 24: action=[2.] reward=-1 done=False\n",
      "Step 25: action=[2.] reward=-1 done=False\n",
      "Step 26: action=[2.] reward=-1 done=False\n",
      "Step 27: action=[2.] reward=-1 done=False\n",
      "Step 28: action=[2.] reward=-1 done=False\n",
      "Step 29: action=[2.] reward=-1 done=False\n",
      "Step 30: action=[2.] reward=-1 done=False\n",
      "Step 31: action=[2.] reward=-1 done=False\n",
      "Step 32: action=[2.] reward=-1 done=False\n",
      "Step 33: action=[2.] reward=-1 done=False\n",
      "Step 34: action=[2.] reward=-1 done=False\n",
      "Step 35: action=[2.] reward=-1 done=False\n",
      "Step 36: action=[2.] reward=999 done=True\n",
      "End of episode 0 with cumulated_reward 963\n",
      "====> Start episode 1\n",
      "Step 0: action=[2.] reward=-1 done=False\n",
      "Step 1: action=[2.] reward=-1 done=False\n",
      "Step 2: action=[2.] reward=-1 done=False\n",
      "Step 3: action=[2.] reward=-1 done=False\n",
      "Step 4: action=[2.] reward=-1 done=False\n",
      "Step 5: action=[2.] reward=-1 done=False\n",
      "Step 6: action=[2.] reward=-1 done=False\n",
      "Step 7: action=[1.9999956] reward=-1 done=False\n",
      "Step 8: action=[1.999197] reward=-1 done=False\n",
      "Step 9: action=[1.8776925] reward=-1 done=False\n",
      "Step 10: action=[-0.3645728] reward=-1 done=False\n",
      "Step 11: action=[-1.933228] reward=-1 done=False\n",
      "Step 12: action=[-1.9979511] reward=-1 done=False\n",
      "Step 13: action=[-1.9999032] reward=-1 done=False\n",
      "Step 14: action=[-1.9999945] reward=-1 done=False\n",
      "Step 15: action=[-1.9999996] reward=-1 done=False\n",
      "Step 16: action=[-1.9999999] reward=-1 done=False\n",
      "Step 17: action=[-1.9999999] reward=-1 done=False\n",
      "Step 18: action=[-1.9999994] reward=-1 done=False\n",
      "Step 19: action=[-1.9999881] reward=-1 done=False\n",
      "Step 20: action=[-1.9996821] reward=-1 done=False\n",
      "Step 21: action=[-1.9849256] reward=-1 done=False\n",
      "Step 22: action=[-1.0711815] reward=-1 done=False\n",
      "Step 23: action=[1.7682002] reward=-1 done=False\n",
      "Step 24: action=[1.9870919] reward=-1 done=False\n",
      "Step 25: action=[1.9998382] reward=-1 done=False\n",
      "Step 26: action=[1.9999957] reward=-1 done=False\n",
      "Step 27: action=[1.9999999] reward=-1 done=False\n",
      "Step 28: action=[2.] reward=-1 done=False\n",
      "Step 29: action=[2.] reward=-1 done=False\n",
      "Step 30: action=[2.] reward=-1 done=False\n",
      "Step 31: action=[2.] reward=-1 done=False\n",
      "Step 32: action=[2.] reward=-1 done=False\n",
      "Step 33: action=[2.] reward=-1 done=False\n",
      "Step 34: action=[2.] reward=-1 done=False\n",
      "Step 35: action=[2.] reward=-1 done=False\n",
      "Step 36: action=[2.] reward=-1 done=False\n",
      "Step 37: action=[2.] reward=-1 done=False\n",
      "Step 38: action=[2.] reward=-1 done=False\n",
      "Step 39: action=[2.] reward=-1 done=False\n",
      "Step 40: action=[2.] reward=-1 done=False\n",
      "Step 41: action=[2.] reward=-1 done=False\n",
      "Step 42: action=[2.] reward=-1 done=False\n",
      "Step 43: action=[2.] reward=-1 done=False\n",
      "Step 44: action=[2.] reward=999 done=True\n",
      "End of episode 1 with cumulated_reward 955\n",
      "====> Start episode 2\n",
      "Step 0: action=[-2.] reward=-1 done=False\n",
      "Step 1: action=[-2.] reward=-1 done=False\n",
      "Step 2: action=[-2.] reward=-1 done=False\n",
      "Step 3: action=[-2.] reward=-1 done=False\n",
      "Step 4: action=[-2.] reward=-1 done=False\n",
      "Step 5: action=[-2.] reward=-1 done=False\n",
      "Step 6: action=[-2.] reward=-1 done=False\n",
      "Step 7: action=[-2.] reward=-1 done=False\n",
      "Step 8: action=[-2.] reward=-1 done=False\n",
      "Step 9: action=[-2.] reward=-1 done=False\n",
      "Step 10: action=[-2.] reward=-1 done=False\n",
      "Step 11: action=[-2.] reward=-1 done=False\n",
      "Step 12: action=[-2.] reward=-1 done=False\n",
      "Step 13: action=[-2.] reward=-1 done=False\n",
      "Step 14: action=[-1.9999998] reward=-1 done=False\n",
      "Step 15: action=[-1.9491448] reward=-1 done=False\n",
      "Step 16: action=[1.9992039] reward=-1 done=False\n",
      "Step 17: action=[1.9999992] reward=-1 done=False\n",
      "Step 18: action=[2.] reward=-1 done=False\n",
      "Step 19: action=[2.] reward=-1 done=False\n",
      "Step 20: action=[2.] reward=-1 done=False\n",
      "Step 21: action=[2.] reward=-1 done=False\n",
      "Step 22: action=[2.] reward=-1 done=False\n",
      "Step 23: action=[2.] reward=-1 done=False\n",
      "Step 24: action=[2.] reward=-1 done=False\n",
      "Step 25: action=[2.] reward=-1 done=False\n",
      "Step 26: action=[2.] reward=-1 done=False\n",
      "Step 27: action=[2.] reward=-1 done=False\n",
      "Step 28: action=[2.] reward=-1 done=False\n",
      "Step 29: action=[2.] reward=-1 done=False\n",
      "Step 30: action=[2.] reward=-1 done=False\n",
      "Step 31: action=[2.] reward=-1 done=False\n",
      "Step 32: action=[2.] reward=-1 done=False\n",
      "Step 33: action=[2.] reward=-1 done=False\n",
      "Step 34: action=[2.] reward=-1 done=False\n",
      "Step 35: action=[2.] reward=-1 done=False\n",
      "Step 36: action=[2.] reward=-1 done=False\n",
      "Step 37: action=[2.] reward=-1 done=False\n",
      "Step 38: action=[2.] reward=-1 done=False\n",
      "Step 39: action=[2.] reward=-1 done=False\n",
      "Step 40: action=[2.] reward=-1 done=False\n",
      "Step 41: action=[2.] reward=-1 done=False\n",
      "Step 42: action=[2.] reward=-1 done=False\n",
      "Step 43: action=[2.] reward=-1 done=False\n",
      "Step 44: action=[2.] reward=-1 done=False\n",
      "Step 45: action=[2.] reward=-1 done=False\n",
      "Step 46: action=[2.] reward=-1 done=False\n",
      "Step 47: action=[2.] reward=-1 done=False\n",
      "Step 48: action=[2.] reward=-1 done=False\n",
      "Step 49: action=[2.] reward=-1 done=False\n",
      "Step 50: action=[2.] reward=-1 done=False\n",
      "Step 51: action=[2.] reward=999 done=True\n",
      "End of episode 2 with cumulated_reward 948\n",
      "====> Start episode 3\n",
      "Step 0: action=[2.] reward=-1 done=False\n",
      "Step 1: action=[2.] reward=-1 done=False\n",
      "Step 2: action=[2.] reward=-1 done=False\n",
      "Step 3: action=[2.] reward=-1 done=False\n",
      "Step 4: action=[2.] reward=-1 done=False\n",
      "Step 5: action=[2.] reward=-1 done=False\n",
      "Step 6: action=[2.] reward=-1 done=False\n",
      "Step 7: action=[2.] reward=-1 done=False\n",
      "Step 8: action=[2.] reward=-1 done=False\n",
      "Step 9: action=[2.] reward=-1 done=False\n",
      "Step 10: action=[2.] reward=-1 done=False\n",
      "Step 11: action=[2.] reward=-1 done=False\n",
      "Step 12: action=[2.] reward=-1 done=False\n",
      "Step 13: action=[2.] reward=-1 done=False\n",
      "Step 14: action=[2.] reward=-1 done=False\n",
      "Step 15: action=[2.] reward=-1 done=False\n",
      "Step 16: action=[2.] reward=-1 done=False\n",
      "Step 17: action=[2.] reward=-1 done=False\n",
      "Step 18: action=[2.] reward=-1 done=False\n",
      "Step 19: action=[2.] reward=-1 done=False\n",
      "Step 20: action=[2.] reward=-1 done=False\n",
      "Step 21: action=[2.] reward=-1 done=False\n",
      "Step 22: action=[2.] reward=-1 done=False\n",
      "Step 23: action=[2.] reward=-1 done=False\n",
      "Step 24: action=[2.] reward=-1 done=False\n",
      "Step 25: action=[2.] reward=-1 done=False\n",
      "Step 26: action=[2.] reward=-1 done=False\n",
      "Step 27: action=[2.] reward=-1 done=False\n",
      "Step 28: action=[2.] reward=-1 done=False\n",
      "Step 29: action=[2.] reward=-1 done=False\n",
      "Step 30: action=[2.] reward=-1 done=False\n",
      "Step 31: action=[2.] reward=-1 done=False\n",
      "Step 32: action=[2.] reward=-1 done=False\n",
      "Step 33: action=[2.] reward=-1 done=False\n",
      "Step 34: action=[2.] reward=-1 done=False\n",
      "Step 35: action=[2.] reward=-1 done=False\n",
      "Step 36: action=[2.] reward=999 done=True\n",
      "End of episode 3 with cumulated_reward 963\n",
      "====> Start episode 4\n",
      "Step 0: action=[-2.] reward=-1 done=False\n",
      "Step 1: action=[-2.] reward=-1 done=False\n",
      "Step 2: action=[-2.] reward=-1 done=False\n",
      "Step 3: action=[-2.] reward=-1 done=False\n",
      "Step 4: action=[-2.] reward=-1 done=False\n",
      "Step 5: action=[-1.9999999] reward=-1 done=False\n",
      "Step 6: action=[-1.9999998] reward=-1 done=False\n",
      "Step 7: action=[-1.9999998] reward=-1 done=False\n",
      "Step 8: action=[-2.] reward=-1 done=False\n",
      "Step 9: action=[-2.] reward=-1 done=False\n",
      "Step 10: action=[-2.] reward=-1 done=False\n",
      "Step 11: action=[-2.] reward=-1 done=False\n",
      "Step 12: action=[-2.] reward=-1 done=False\n",
      "Step 13: action=[-2.] reward=-1 done=False\n",
      "Step 14: action=[-2.] reward=-1 done=False\n",
      "Step 15: action=[-2.] reward=-1 done=False\n",
      "Step 16: action=[-2.] reward=-1 done=False\n",
      "Step 17: action=[-1.9999878] reward=-1 done=False\n",
      "Step 18: action=[-1.9908427] reward=-1 done=False\n",
      "Step 19: action=[0.734912] reward=-1 done=False\n",
      "Step 20: action=[1.998694] reward=-1 done=False\n",
      "Step 21: action=[1.9999982] reward=-1 done=False\n",
      "Step 22: action=[2.] reward=-1 done=False\n",
      "Step 23: action=[2.] reward=-1 done=False\n",
      "Step 24: action=[2.] reward=-1 done=False\n",
      "Step 25: action=[2.] reward=-1 done=False\n",
      "Step 26: action=[2.] reward=-1 done=False\n",
      "Step 27: action=[2.] reward=-1 done=False\n",
      "Step 28: action=[2.] reward=-1 done=False\n",
      "Step 29: action=[2.] reward=-1 done=False\n",
      "Step 30: action=[2.] reward=-1 done=False\n",
      "Step 31: action=[2.] reward=-1 done=False\n",
      "Step 32: action=[2.] reward=-1 done=False\n",
      "Step 33: action=[2.] reward=-1 done=False\n",
      "Step 34: action=[2.] reward=-1 done=False\n",
      "Step 35: action=[2.] reward=-1 done=False\n",
      "Step 36: action=[2.] reward=-1 done=False\n",
      "Step 37: action=[2.] reward=-1 done=False\n",
      "Step 38: action=[2.] reward=-1 done=False\n",
      "Step 39: action=[2.] reward=-1 done=False\n",
      "Step 40: action=[2.] reward=-1 done=False\n",
      "Step 41: action=[2.] reward=-1 done=False\n",
      "Step 42: action=[2.] reward=-1 done=False\n",
      "Step 43: action=[2.] reward=-1 done=False\n",
      "Step 44: action=[2.] reward=-1 done=False\n",
      "Step 45: action=[2.] reward=-1 done=False\n",
      "Step 46: action=[2.] reward=-1 done=False\n",
      "Step 47: action=[2.] reward=-1 done=False\n",
      "Step 48: action=[2.] reward=-1 done=False\n",
      "Step 49: action=[2.] reward=-1 done=False\n",
      "Step 50: action=[2.] reward=-1 done=False\n",
      "Step 51: action=[2.] reward=-1 done=False\n",
      "Step 52: action=[2.] reward=-1 done=False\n",
      "Step 53: action=[2.] reward=-1 done=False\n",
      "Step 54: action=[2.] reward=999 done=True\n",
      "End of episode 4 with cumulated_reward 945\n",
      "====> Start episode 5\n",
      "Step 0: action=[1.9999605] reward=-1 done=False\n",
      "Step 1: action=[1.9983711] reward=-1 done=False\n",
      "Step 2: action=[1.9349937] reward=-1 done=False\n",
      "Step 3: action=[0.2576312] reward=-1 done=False\n",
      "Step 4: action=[-1.8186102] reward=-1 done=False\n",
      "Step 5: action=[-1.9930311] reward=-1 done=False\n",
      "Step 6: action=[-1.9996061] reward=-1 done=False\n",
      "Step 7: action=[-1.9999828] reward=-1 done=False\n",
      "Step 8: action=[-1.9999994] reward=-1 done=False\n",
      "Step 9: action=[-2.] reward=-1 done=False\n",
      "Step 10: action=[-2.] reward=-1 done=False\n",
      "Step 11: action=[-2.] reward=-1 done=False\n",
      "Step 12: action=[-2.] reward=-1 done=False\n",
      "Step 13: action=[-2.] reward=-1 done=False\n",
      "Step 14: action=[-2.] reward=-1 done=False\n",
      "Step 15: action=[-2.] reward=-1 done=False\n",
      "Step 16: action=[-2.] reward=-1 done=False\n",
      "Step 17: action=[-2.] reward=-1 done=False\n",
      "Step 18: action=[-2.] reward=-1001 done=True\n",
      "End of episode 5 with cumulated_reward -1019\n",
      "====> Start episode 6\n",
      "Step 0: action=[2.] reward=-1 done=False\n",
      "Step 1: action=[2.] reward=-1 done=False\n",
      "Step 2: action=[2.] reward=-1 done=False\n",
      "Step 3: action=[2.] reward=-1 done=False\n",
      "Step 4: action=[2.] reward=-1 done=False\n",
      "Step 5: action=[2.] reward=-1 done=False\n",
      "Step 6: action=[2.] reward=-1 done=False\n",
      "Step 7: action=[2.] reward=-1 done=False\n",
      "Step 8: action=[2.] reward=-1 done=False\n",
      "Step 9: action=[2.] reward=-1 done=False\n",
      "Step 10: action=[2.] reward=-1 done=False\n",
      "Step 11: action=[2.] reward=-1 done=False\n",
      "Step 12: action=[2.] reward=-1 done=False\n",
      "Step 13: action=[2.] reward=-1 done=False\n",
      "Step 14: action=[2.] reward=-1 done=False\n",
      "Step 15: action=[2.] reward=-1 done=False\n",
      "Step 16: action=[2.] reward=-1 done=False\n",
      "Step 17: action=[2.] reward=-1 done=False\n",
      "Step 18: action=[2.] reward=-1 done=False\n",
      "Step 19: action=[2.] reward=-1 done=False\n",
      "Step 20: action=[2.] reward=-1 done=False\n",
      "Step 21: action=[2.] reward=-1 done=False\n",
      "Step 22: action=[2.] reward=-1 done=False\n",
      "Step 23: action=[2.] reward=-1 done=False\n",
      "Step 24: action=[2.] reward=-1 done=False\n",
      "Step 25: action=[2.] reward=-1 done=False\n",
      "Step 26: action=[2.] reward=-1 done=False\n",
      "Step 27: action=[2.] reward=-1 done=False\n",
      "Step 28: action=[2.] reward=-1 done=False\n",
      "Step 29: action=[2.] reward=-1 done=False\n",
      "Step 30: action=[2.] reward=-1 done=False\n",
      "Step 31: action=[2.] reward=-1 done=False\n",
      "Step 32: action=[2.] reward=-1 done=False\n",
      "Step 33: action=[2.] reward=-1 done=False\n",
      "Step 34: action=[2.] reward=-1 done=False\n",
      "Step 35: action=[2.] reward=-1 done=False\n",
      "Step 36: action=[2.] reward=999 done=True\n",
      "End of episode 6 with cumulated_reward 963\n",
      "====> Start episode 7\n",
      "Step 0: action=[2.] reward=-1 done=False\n",
      "Step 1: action=[2.] reward=-1 done=False\n",
      "Step 2: action=[2.] reward=-1 done=False\n",
      "Step 3: action=[2.] reward=-1 done=False\n",
      "Step 4: action=[2.] reward=-1 done=False\n",
      "Step 5: action=[2.] reward=-1 done=False\n",
      "Step 6: action=[2.] reward=-1 done=False\n",
      "Step 7: action=[2.] reward=-1 done=False\n",
      "Step 8: action=[2.] reward=-1 done=False\n",
      "Step 9: action=[1.9999999] reward=-1 done=False\n",
      "Step 10: action=[1.9999936] reward=-1 done=False\n",
      "Step 11: action=[1.9997188] reward=-1 done=False\n",
      "Step 12: action=[1.9958559] reward=-1 done=False\n",
      "Step 13: action=[1.9912012] reward=-1 done=False\n",
      "Step 14: action=[1.9939177] reward=-1 done=False\n",
      "Step 15: action=[1.9975004] reward=-1 done=False\n",
      "Step 16: action=[1.999956] reward=-1 done=False\n",
      "Step 17: action=[1.9999998] reward=-1 done=False\n",
      "Step 18: action=[2.] reward=-1 done=False\n",
      "Step 19: action=[2.] reward=-1 done=False\n",
      "Step 20: action=[2.] reward=-1 done=False\n",
      "Step 21: action=[2.] reward=-1 done=False\n",
      "Step 22: action=[2.] reward=-1 done=False\n",
      "Step 23: action=[2.] reward=-1 done=False\n",
      "Step 24: action=[2.] reward=-1 done=False\n",
      "Step 25: action=[2.] reward=-1 done=False\n",
      "Step 26: action=[2.] reward=-1 done=False\n",
      "Step 27: action=[2.] reward=-1 done=False\n",
      "Step 28: action=[2.] reward=-1 done=False\n",
      "Step 29: action=[2.] reward=-1 done=False\n",
      "Step 30: action=[2.] reward=-1 done=False\n",
      "Step 31: action=[2.] reward=-1 done=False\n",
      "Step 32: action=[2.] reward=-1 done=False\n",
      "Step 33: action=[2.] reward=-1 done=False\n",
      "Step 34: action=[2.] reward=-1 done=False\n",
      "Step 35: action=[2.] reward=-1 done=False\n",
      "Step 36: action=[2.] reward=999 done=True\n",
      "End of episode 7 with cumulated_reward 963\n",
      "====> Start episode 8\n",
      "Step 0: action=[2.] reward=-1 done=False\n",
      "Step 1: action=[2.] reward=-1 done=False\n",
      "Step 2: action=[2.] reward=-1 done=False\n",
      "Step 3: action=[2.] reward=-1 done=False\n",
      "Step 4: action=[2.] reward=-1 done=False\n",
      "Step 5: action=[2.] reward=-1 done=False\n",
      "Step 6: action=[2.] reward=-1 done=False\n",
      "Step 7: action=[2.] reward=-1 done=False\n",
      "Step 8: action=[2.] reward=-1 done=False\n",
      "Step 9: action=[2.] reward=-1 done=False\n",
      "Step 10: action=[2.] reward=-1 done=False\n",
      "Step 11: action=[2.] reward=-1 done=False\n",
      "Step 12: action=[2.] reward=-1 done=False\n",
      "Step 13: action=[2.] reward=-1 done=False\n",
      "Step 14: action=[2.] reward=-1 done=False\n",
      "Step 15: action=[2.] reward=-1 done=False\n",
      "Step 16: action=[2.] reward=-1 done=False\n",
      "Step 17: action=[2.] reward=-1 done=False\n",
      "Step 18: action=[2.] reward=-1 done=False\n",
      "Step 19: action=[2.] reward=-1 done=False\n",
      "Step 20: action=[2.] reward=-1 done=False\n",
      "Step 21: action=[2.] reward=-1 done=False\n",
      "Step 22: action=[2.] reward=-1 done=False\n",
      "Step 23: action=[2.] reward=-1 done=False\n",
      "Step 24: action=[2.] reward=-1 done=False\n",
      "Step 25: action=[2.] reward=-1 done=False\n",
      "Step 26: action=[2.] reward=-1 done=False\n",
      "Step 27: action=[2.] reward=-1 done=False\n",
      "Step 28: action=[2.] reward=-1 done=False\n",
      "Step 29: action=[2.] reward=-1 done=False\n",
      "Step 30: action=[2.] reward=-1 done=False\n",
      "Step 31: action=[2.] reward=-1 done=False\n",
      "Step 32: action=[2.] reward=-1 done=False\n",
      "Step 33: action=[2.] reward=-1 done=False\n",
      "Step 34: action=[2.] reward=-1 done=False\n",
      "Step 35: action=[2.] reward=-1 done=False\n",
      "Step 36: action=[2.] reward=999 done=True\n",
      "End of episode 8 with cumulated_reward 963\n",
      "====> Start episode 9\n",
      "Step 0: action=[2.] reward=-1 done=False\n",
      "Step 1: action=[2.] reward=-1 done=False\n",
      "Step 2: action=[2.] reward=-1 done=False\n",
      "Step 3: action=[2.] reward=-1 done=False\n",
      "Step 4: action=[2.] reward=-1 done=False\n",
      "Step 5: action=[2.] reward=-1 done=False\n",
      "Step 6: action=[2.] reward=-1 done=False\n",
      "Step 7: action=[2.] reward=-1 done=False\n",
      "Step 8: action=[2.] reward=-1 done=False\n",
      "Step 9: action=[2.] reward=-1 done=False\n",
      "Step 10: action=[2.] reward=-1 done=False\n",
      "Step 11: action=[2.] reward=-1 done=False\n",
      "Step 12: action=[2.] reward=-1 done=False\n",
      "Step 13: action=[2.] reward=-1 done=False\n",
      "Step 14: action=[2.] reward=-1 done=False\n",
      "Step 15: action=[2.] reward=-1 done=False\n",
      "Step 16: action=[2.] reward=-1 done=False\n",
      "Step 17: action=[2.] reward=-1 done=False\n",
      "Step 18: action=[2.] reward=-1 done=False\n",
      "Step 19: action=[2.] reward=-1 done=False\n",
      "Step 20: action=[2.] reward=-1 done=False\n",
      "Step 21: action=[2.] reward=-1 done=False\n",
      "Step 22: action=[2.] reward=-1 done=False\n",
      "Step 23: action=[2.] reward=-1 done=False\n",
      "Step 24: action=[2.] reward=-1 done=False\n",
      "Step 25: action=[2.] reward=-1 done=False\n",
      "Step 26: action=[2.] reward=-1 done=False\n",
      "Step 27: action=[2.] reward=-1 done=False\n",
      "Step 28: action=[2.] reward=-1 done=False\n",
      "Step 29: action=[2.] reward=-1 done=False\n",
      "Step 30: action=[2.] reward=-1 done=False\n",
      "Step 31: action=[2.] reward=-1 done=False\n",
      "Step 32: action=[2.] reward=-1 done=False\n",
      "Step 33: action=[2.] reward=-1 done=False\n",
      "Step 34: action=[2.] reward=-1 done=False\n",
      "Step 35: action=[2.] reward=-1 done=False\n",
      "Step 36: action=[2.] reward=999 done=True\n",
      "End of episode 9 with cumulated_reward 963\n",
      "====> Start episode 10\n",
      "Step 0: action=[2.] reward=-1 done=False\n",
      "Step 1: action=[2.] reward=-1 done=False\n",
      "Step 2: action=[2.] reward=-1 done=False\n",
      "Step 3: action=[2.] reward=-1 done=False\n",
      "Step 4: action=[2.] reward=-1 done=False\n",
      "Step 5: action=[2.] reward=-1 done=False\n",
      "Step 6: action=[2.] reward=-1 done=False\n",
      "Step 7: action=[2.] reward=-1 done=False\n",
      "Step 8: action=[2.] reward=-1 done=False\n",
      "Step 9: action=[2.] reward=-1 done=False\n",
      "Step 10: action=[2.] reward=-1 done=False\n",
      "Step 11: action=[2.] reward=-1 done=False\n",
      "Step 12: action=[2.] reward=-1 done=False\n",
      "Step 13: action=[2.] reward=-1 done=False\n",
      "Step 14: action=[2.] reward=-1 done=False\n",
      "Step 15: action=[2.] reward=-1 done=False\n",
      "Step 16: action=[2.] reward=-1 done=False\n",
      "Step 17: action=[2.] reward=-1 done=False\n",
      "Step 18: action=[2.] reward=-1 done=False\n",
      "Step 19: action=[2.] reward=-1 done=False\n",
      "Step 20: action=[2.] reward=-1 done=False\n",
      "Step 21: action=[2.] reward=-1 done=False\n",
      "Step 22: action=[2.] reward=-1 done=False\n",
      "Step 23: action=[2.] reward=-1 done=False\n",
      "Step 24: action=[2.] reward=-1 done=False\n",
      "Step 25: action=[2.] reward=-1 done=False\n",
      "Step 26: action=[2.] reward=-1 done=False\n",
      "Step 27: action=[2.] reward=-1 done=False\n",
      "Step 28: action=[2.] reward=-1 done=False\n",
      "Step 29: action=[2.] reward=-1 done=False\n",
      "Step 30: action=[2.] reward=-1 done=False\n",
      "Step 31: action=[2.] reward=-1 done=False\n",
      "Step 32: action=[2.] reward=-1 done=False\n",
      "Step 33: action=[2.] reward=-1 done=False\n",
      "Step 34: action=[2.] reward=-1 done=False\n",
      "Step 35: action=[2.] reward=-1 done=False\n",
      "Step 36: action=[2.] reward=999 done=True\n",
      "End of episode 10 with cumulated_reward 963\n",
      "====> Start episode 11\n",
      "Step 0: action=[2.] reward=-1 done=False\n",
      "Step 1: action=[2.] reward=-1 done=False\n",
      "Step 2: action=[2.] reward=-1 done=False\n",
      "Step 3: action=[2.] reward=-1 done=False\n",
      "Step 4: action=[2.] reward=-1 done=False\n",
      "Step 5: action=[2.] reward=-1 done=False\n",
      "Step 6: action=[2.] reward=-1 done=False\n",
      "Step 7: action=[2.] reward=-1 done=False\n",
      "Step 8: action=[2.] reward=-1 done=False\n",
      "Step 9: action=[2.] reward=-1 done=False\n",
      "Step 10: action=[2.] reward=-1 done=False\n",
      "Step 11: action=[2.] reward=-1 done=False\n",
      "Step 12: action=[2.] reward=-1 done=False\n",
      "Step 13: action=[2.] reward=-1 done=False\n",
      "Step 14: action=[2.] reward=-1 done=False\n",
      "Step 15: action=[2.] reward=-1 done=False\n",
      "Step 16: action=[2.] reward=-1 done=False\n",
      "Step 17: action=[2.] reward=-1 done=False\n",
      "Step 18: action=[2.] reward=-1 done=False\n",
      "Step 19: action=[2.] reward=-1 done=False\n",
      "Step 20: action=[2.] reward=-1 done=False\n",
      "Step 21: action=[2.] reward=-1 done=False\n",
      "Step 22: action=[2.] reward=-1 done=False\n",
      "Step 23: action=[2.] reward=-1 done=False\n",
      "Step 24: action=[2.] reward=-1 done=False\n",
      "Step 25: action=[2.] reward=-1 done=False\n",
      "Step 26: action=[2.] reward=-1 done=False\n",
      "Step 27: action=[2.] reward=-1 done=False\n",
      "Step 28: action=[2.] reward=-1 done=False\n",
      "Step 29: action=[2.] reward=-1 done=False\n",
      "Step 30: action=[2.] reward=-1 done=False\n",
      "Step 31: action=[2.] reward=-1 done=False\n",
      "Step 32: action=[2.] reward=-1 done=False\n",
      "Step 33: action=[2.] reward=-1 done=False\n",
      "Step 34: action=[2.] reward=-1 done=False\n",
      "Step 35: action=[2.] reward=-1 done=False\n",
      "Step 36: action=[2.] reward=999 done=True\n",
      "End of episode 11 with cumulated_reward 963\n",
      "====> Start episode 12\n",
      "Step 0: action=[-2.] reward=-1 done=False\n",
      "Step 1: action=[-2.] reward=-1 done=False\n",
      "Step 2: action=[-2.] reward=-1 done=False\n",
      "Step 3: action=[-2.] reward=-1 done=False\n",
      "Step 4: action=[-2.] reward=-1 done=False\n",
      "Step 5: action=[-2.] reward=-1 done=False\n",
      "Step 6: action=[-2.] reward=-1 done=False\n",
      "Step 7: action=[-2.] reward=-1 done=False\n",
      "Step 8: action=[-2.] reward=-1 done=False\n",
      "Step 9: action=[-2.] reward=-1 done=False\n",
      "Step 10: action=[-2.] reward=-1 done=False\n",
      "Step 11: action=[-1.9999996] reward=-1 done=False\n",
      "Step 12: action=[-1.9999211] reward=-1 done=False\n",
      "Step 13: action=[-1.9973783] reward=-1 done=False\n",
      "Step 14: action=[-1.8873296] reward=-1 done=False\n",
      "Step 15: action=[1.6655626] reward=-1 done=False\n",
      "Step 16: action=[1.9726406] reward=-1 done=False\n",
      "Step 17: action=[1.9978831] reward=-1 done=False\n",
      "Step 18: action=[1.9999405] reward=-1 done=False\n",
      "Step 19: action=[1.9999985] reward=-1 done=False\n",
      "Step 20: action=[2.] reward=-1 done=False\n",
      "Step 21: action=[2.] reward=-1 done=False\n",
      "Step 22: action=[2.] reward=-1 done=False\n",
      "Step 23: action=[2.] reward=-1 done=False\n",
      "Step 24: action=[2.] reward=-1 done=False\n",
      "Step 25: action=[2.] reward=-1 done=False\n",
      "Step 26: action=[2.] reward=-1 done=False\n",
      "Step 27: action=[2.] reward=-1 done=False\n",
      "Step 28: action=[2.] reward=-1 done=False\n",
      "Step 29: action=[2.] reward=-1 done=False\n",
      "Step 30: action=[2.] reward=-1 done=False\n",
      "Step 31: action=[2.] reward=-1 done=False\n",
      "Step 32: action=[2.] reward=-1 done=False\n",
      "Step 33: action=[2.] reward=-1 done=False\n",
      "Step 34: action=[2.] reward=-1 done=False\n",
      "Step 35: action=[2.] reward=-1 done=False\n",
      "Step 36: action=[2.] reward=-1 done=False\n",
      "Step 37: action=[2.] reward=-1 done=False\n",
      "Step 38: action=[2.] reward=-1 done=False\n",
      "Step 39: action=[2.] reward=-1 done=False\n",
      "Step 40: action=[2.] reward=-1 done=False\n",
      "Step 41: action=[2.] reward=-1 done=False\n",
      "Step 42: action=[2.] reward=-1 done=False\n",
      "Step 43: action=[2.] reward=-1 done=False\n",
      "Step 44: action=[2.] reward=-1 done=False\n",
      "Step 45: action=[2.] reward=-1 done=False\n",
      "Step 46: action=[2.] reward=-1 done=False\n",
      "Step 47: action=[2.] reward=-1 done=False\n",
      "Step 48: action=[2.] reward=-1 done=False\n",
      "Step 49: action=[2.] reward=-1 done=False\n",
      "Step 50: action=[2.] reward=999 done=True\n",
      "End of episode 12 with cumulated_reward 949\n",
      "====> Start episode 13\n",
      "Step 0: action=[1.9999309] reward=-1 done=False\n",
      "Step 1: action=[1.9773332] reward=-1 done=False\n",
      "Step 2: action=[0.28866956] reward=-1 done=False\n",
      "Step 3: action=[-1.8979497] reward=-1 done=False\n",
      "Step 4: action=[-1.9949054] reward=-1 done=False\n",
      "Step 5: action=[-1.9997182] reward=-1 done=False\n",
      "Step 6: action=[-1.9999877] reward=-1 done=False\n",
      "Step 7: action=[-1.9999995] reward=-1 done=False\n",
      "Step 8: action=[-2.] reward=-1 done=False\n",
      "Step 9: action=[-2.] reward=-1 done=False\n",
      "Step 10: action=[-2.] reward=-1 done=False\n",
      "Step 11: action=[-2.] reward=-1 done=False\n",
      "Step 12: action=[-2.] reward=-1001 done=True\n",
      "End of episode 13 with cumulated_reward -1013\n",
      "====> Start episode 14\n",
      "Step 0: action=[2.] reward=-1 done=False\n",
      "Step 1: action=[2.] reward=-1 done=False\n",
      "Step 2: action=[2.] reward=-1 done=False\n",
      "Step 3: action=[2.] reward=-1 done=False\n",
      "Step 4: action=[2.] reward=-1 done=False\n",
      "Step 5: action=[2.] reward=-1 done=False\n",
      "Step 6: action=[2.] reward=-1 done=False\n",
      "Step 7: action=[2.] reward=-1 done=False\n",
      "Step 8: action=[2.] reward=-1 done=False\n",
      "Step 9: action=[2.] reward=-1 done=False\n",
      "Step 10: action=[2.] reward=-1 done=False\n",
      "Step 11: action=[2.] reward=-1 done=False\n",
      "Step 12: action=[2.] reward=-1 done=False\n",
      "Step 13: action=[2.] reward=-1 done=False\n",
      "Step 14: action=[2.] reward=-1 done=False\n",
      "Step 15: action=[2.] reward=-1 done=False\n",
      "Step 16: action=[2.] reward=-1 done=False\n",
      "Step 17: action=[2.] reward=-1 done=False\n",
      "Step 18: action=[2.] reward=-1 done=False\n",
      "Step 19: action=[2.] reward=-1 done=False\n",
      "Step 20: action=[2.] reward=-1 done=False\n",
      "Step 21: action=[2.] reward=-1 done=False\n",
      "Step 22: action=[2.] reward=-1 done=False\n",
      "Step 23: action=[2.] reward=-1 done=False\n",
      "Step 24: action=[2.] reward=-1 done=False\n",
      "Step 25: action=[2.] reward=-1 done=False\n",
      "Step 26: action=[2.] reward=-1 done=False\n",
      "Step 27: action=[2.] reward=-1 done=False\n",
      "Step 28: action=[2.] reward=-1 done=False\n",
      "Step 29: action=[2.] reward=-1 done=False\n",
      "Step 30: action=[2.] reward=-1 done=False\n",
      "Step 31: action=[2.] reward=-1 done=False\n",
      "Step 32: action=[2.] reward=-1 done=False\n",
      "Step 33: action=[2.] reward=-1 done=False\n",
      "Step 34: action=[2.] reward=-1 done=False\n",
      "Step 35: action=[2.] reward=-1 done=False\n",
      "Step 36: action=[2.] reward=999 done=True\n",
      "End of episode 14 with cumulated_reward 963\n",
      "METRICS: REWARD AvgCumulatedReward = 958.77\n",
      "METRICS: SAFETY %collisions = 0.13, COMFORT MeanHardBrake = 3.00, EFFICIENCY MeanStepsToGoal = 40.23\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "max_episodes = 15\n",
    "max_steps = 200\n",
    "\n",
    "# METRICS\n",
    "metric_success = 0 # EFFICIENCY\n",
    "metric_steps_to_goal = [] # SAFETY\n",
    "metric_hardbrake = [] # COMFORT\n",
    "metric_cumulated_reward = []\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    print(\"====> Start episode {}\".format(episode))\n",
    "    state = env.reset()\n",
    "    cumulated_reward = 0\n",
    "    images = []\n",
    "    \n",
    "    hardbrake = 0    \n",
    "    for n in range(max_steps):\n",
    "        #action = 0\n",
    "        action = get_action(state)\n",
    "        #action = np.random.randint(low=-2,high=3) \n",
    "        if action <= -2:\n",
    "            hardbrake += 1\n",
    "        state, reward, done, info = env.step(action)\n",
    "        env.penalty(state)\n",
    "        cumulated_reward += reward\n",
    "        print(\"Step {}: action={} reward={} done={}\".format(n, action, reward, done)) # PHW DEBUG\n",
    "        img = env.render()\n",
    "        images.append(img)\n",
    "        if done is True:\n",
    "            if info == \"success\":\n",
    "                metric_success += 1\n",
    "                metric_steps_to_goal.append(n)\n",
    "                metric_hardbrake.append(hardbrake)\n",
    "                metric_cumulated_reward.append(cumulated_reward)\n",
    "            print(\"End of episode {} with cumulated_reward {}\".format(episode, cumulated_reward))\n",
    "            break\n",
    "\n",
    "print(\"METRICS: REWARD AvgCumulatedReward = {:.2f}\".format(np.mean(metric_cumulated_reward)))\n",
    "print(\"METRICS: SAFETY %collisions = {:.2f}, COMFORT MeanHardBrake = {:.2f}, EFFICIENCY MeanStepsToGoal = {:.2f}\".format(1-metric_success/max_episodes, np.mean(metric_hardbrake), np.mean(metric_steps_to_goal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 14, 0, 14, 0, 0, 0, 0, 0, 0, 11, 0]\n",
      "[36, 44, 51, 36, 54, 36, 36, 36, 36, 36, 36, 50, 36]\n",
      "13\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "print(metric_hardbrake)\n",
    "print(metric_steps_to_goal)\n",
    "print(metric_success)\n",
    "print(max_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_img(images[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imageio.mimsave('img/visu.gif', images, duration=0.2)\n",
    "HTML(\"\"\"<img src=\"img/visu.gif\"/>\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "big_images = resize_images(images, f=2)\n",
    "imageio.mimsave('img/visu2.gif', big_images, duration=0.2)\n",
    "HTML(\"\"\"<img src=\"img/visu2.gif\"/>\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
